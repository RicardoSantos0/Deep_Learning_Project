{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project\n",
    "###### The current project relies on using CNNs in order to process sperm data. \n",
    "\n",
    "The goal is to develop a classifier able to identify different morphologies of head spearm cells.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONTEXT :\n",
    "\n",
    "Due to our background in the areas of biology and health, it was in our interest to use this project to develop our knowledge of neural network, in microscopic images.\n",
    "The chosen theme is a classification problem taking into account the morphology of the sperm cell heads.\n",
    "\n",
    " #### There are two datasets available, which we have chosen to aggregate into one.\n",
    "\n",
    "- SCIAN-MorphoSpermGS  - https://cimt.uchile.cl/gold10/ -  with 1132 image, each image has 35 x 35 pixels and has been classified by 3 experts. We will use majority vote result as target. \n",
    "\n",
    "\n",
    "- Human Sperm head Morphology dataset – HuSHeM- https://easy.dans.knaw.nl/ui/datasets/id/easy-dataset:77214/tab/2 -  with 725 images were taken, each image has 576×720 pixels and have the respective classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>  The heads of sperm cells will have the following classification:</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![Title](Heads.png)\n",
    "\n",
    "\n",
    "* We don't include the \"small\" class ! \n",
    "\n",
    "Ref: https://doi.org/10.1016/j.compbiomed.2019.103342"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> The process of adapting the data for use on CNN for reasons of operating system compatibility has been removed. But they can be found in [this document.][mylink]\n",
    "\n",
    "[mylink]: DL_Project_LoadImg.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### In this pipeline, 4 models were developed:\n",
    "\n",
    "- The first model was produced by us, based on heuristics.\n",
    "- The second model was developed based on the Alex Net network architecture\n",
    "- The third model was based on the Resnet50 model.\n",
    "- The last model was based on an Efficient Net which, according to the current literature, has very promising results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#base libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "\n",
    "#image manipulation packages\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "\n",
    "#Classification\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix\n",
    "\n",
    "sns.set()\n",
    "\n",
    "#import keras\n",
    "from tensorflow import keras\n",
    "import glob\n",
    "import cv2\n",
    "import shutil\n",
    "import random\n",
    "import Augmentor\n",
    "\n",
    "#Data Augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Callbacks\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "## RESNET50\n",
    "\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "\n",
    "### Efficient\n",
    "from tensorflow.keras.applications import EfficientNetB7\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Import Images and Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=pd.read_csv(\"Majority_Vote.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../CNN_SpermCells')\n",
    "path = os.getcwd()\n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/joanarafael/Documentos/NOVA_IMS/Data Science/2S_DataScience/WorkGroup/Deep Learning/CNN_SpermCells'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images Normal: 3824\n",
      "Images Tapered: 281\n",
      "Images Pyriform: 133\n",
      "Images Amorphous: 708\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Count the number of images by class\n",
    "\n",
    "def fileCount(folder):\n",
    "    \"count the number of files in a directory\"\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        path = os.path.join(folder, filename)\n",
    "\n",
    "        if os.path.isfile(path):\n",
    "            count += 1\n",
    "        elif os.path.isdir(path):\n",
    "            count += fileCount(path)\n",
    "\n",
    "    return count\n",
    "\n",
    "count_0 = fileCount(path+'/path/class0')\n",
    "count_1 = fileCount(path+'/path/class1')\n",
    "count_2 = fileCount(path+'/path/class2')\n",
    "count_3 = fileCount(path+'/path/class3')\n",
    "\n",
    "print(f'Images Normal: {count_0}\\n' +\n",
    "     f'Images Tapered: {count_1}\\n' +\n",
    "     f'Images Pyriform: {count_2}\\n' +\n",
    "     f'Images Amorphous: {count_3}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.1 DATA AUGMENTATION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Offline Data Augmentation\n",
    "\n",
    "Options are limited since we're working with microscopy data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Executing Pipeline: 0 Samples [00:00, ? Samples/s]\n",
      "Executing Pipeline:   0%|          | 0/719 [00:00<?, ? Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 92 image(s) found.\n",
      "Output directory set to /Users/joanarafael/Documentos/NOVA_IMS/Data Science/2S_DataScience/WorkGroup/Deep Learning/CNN_SpermCells/path/train/class0/output.Initialised with 168 image(s) found.\n",
      "Output directory set to /Users/joanarafael/Documentos/NOVA_IMS/Data Science/2S_DataScience/WorkGroup/Deep Learning/CNN_SpermCells/path/train/class1/output.Initialised with 79 image(s) found.\n",
      "Output directory set to /Users/joanarafael/Documentos/NOVA_IMS/Data Science/2S_DataScience/WorkGroup/Deep Learning/CNN_SpermCells/path/train/class2/output.Initialised with 424 image(s) found.\n",
      "Output directory set to /Users/joanarafael/Documentos/NOVA_IMS/Data Science/2S_DataScience/WorkGroup/Deep Learning/CNN_SpermCells/path/train/class3/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.TiffImagePlugin.TiffImageFile image mode=L size=35x35 at 0x7FADFA2A8760>: 100%|██████████| 719/719 [00:12<00:00, 59.91 Samples/s]  \n",
      "Processing <PIL.Image.Image image mode=RGB size=131x131 at 0x7FADFA3DCEE0>: 100%|██████████| 867/867 [00:12<00:00, 70.59 Samples/s]                \n",
      "Processing <PIL.Image.Image image mode=L size=35x35 at 0x7FADBA9892B0>: 100%|██████████| 292/292 [00:05<00:00, 51.55 Samples/s]                    \n"
     ]
    }
   ],
   "source": [
    "# Define augmentation pipelines\n",
    "class_0 = Augmentor.Pipeline(path+'/path/train/class0')\n",
    "class_1 = Augmentor.Pipeline(path+'/path/train/class1')\n",
    "class_2 = Augmentor.Pipeline(path+'/path/train/class2')\n",
    "class_3 = Augmentor.Pipeline(path+'/path/train/class3')\n",
    "\n",
    "\n",
    "#rotate by a maximum of 90 degrees\n",
    "class_0.rotate(probability=0.7, max_left_rotation=25, max_right_rotation=25)\n",
    "class_1.rotate(probability=0.7, max_left_rotation=25, max_right_rotation=25)\n",
    "class_2.rotate(probability=0.7, max_left_rotation=25, max_right_rotation=25)\n",
    "class_3.rotate(probability=0.7, max_left_rotation=25, max_right_rotation=25)\n",
    "\n",
    "#mirroring, vertical or horizontal, randomly\n",
    "class_0.flip_random(probability=0.7)\n",
    "class_1.flip_random(probability=0.7)\n",
    "class_2.flip_random(probability=0.7)\n",
    "class_3.flip_random(probability=0.7)\n",
    "\n",
    "\n",
    "# Augment images to the same proportion as existing ones in class 4 (majority class - get to 1000 in each class)\n",
    "class_0.sample(6400 - count_0)\n",
    "class_1.sample(6400 - count_1)\n",
    "class_2.sample(6400 - count_2)\n",
    "class_3.sample(6400 - count_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Define Callbacks\n",
    "- Checkpoint\n",
    "- Early Stopping\n",
    "- Reduce Learning Rate\n",
    "- Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = path+'/Models/'\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0, verbose = 1)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "\n",
    "mc = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    verbose = 1,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "class TimingCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, logs={}):\n",
    "        self.logs=[]\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.starttime = timer()\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(timer()-self.starttime)\n",
    "\n",
    "cb = TimingCallback()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models:\n",
    "- <a href='#1'>First Model</a>\n",
    "- <a href='#2'>Alex Net</a>\n",
    "- <a href='#3'>ResNet50</a>\n",
    "- <a href='#4'>EfficientNet B7</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. First Model \n",
    "<a id='1'></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = path+'/Models/model_HM.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Preprocess input\n",
    "Online Data Augmentation  in GrayScale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11217 images belonging to 4 classes.\n",
      "Found 256 images belonging to 4 classes.\n",
      "Found 257 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cnn_train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   rotation_range = 5,\n",
    "                                   width_shift_range = 0.1,\n",
    "                                   height_shift_range = 0.1, \n",
    "                                   vertical_flip = True,\n",
    "                                   horizontal_flip = True,\n",
    "                                   brightness_range=[0.2,1.5], \n",
    "                                   fill_mode='nearest',\n",
    "                                   zoom_range = 0.2,\n",
    "                                   ) \n",
    "\n",
    "cnn_val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "cnn_test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "#test different color maps -  class modes and cross validation types\n",
    "cnn_training = cnn_train_datagen.flow_from_directory(path+'/path/train',\n",
    "                                                 target_size = (32, 32),\n",
    "                                                 batch_size = 64,\n",
    "                                                 shuffle = True,\n",
    "                                                 class_mode=\"categorical\",\n",
    "                                                 color_mode = 'grayscale')\n",
    "\n",
    "cnn_val = cnn_val_datagen.flow_from_directory(path+'/path/val',\n",
    "                                            target_size = (32, 32),\n",
    "                                            batch_size = 64,\n",
    "                                            shuffle = True,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            color_mode = 'grayscale')\n",
    "\n",
    "cnn_test = cnn_test_datagen.flow_from_directory(path+'/path/test',\n",
    "                                            target_size = (32, 32),\n",
    "                                            batch_size = 1,\n",
    "                                            shuffle = True,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            color_mode = 'grayscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnn_model = Sequential()\n",
    "\n",
    "#convolutional layer with 32 3x3 filters - again, arbitrary\n",
    "\n",
    "\n",
    "cnn_model.add(Conv2D(filters=32, kernel_size=(3,3), padding = 'same', activation='relu')) \n",
    "cnn_model.add(BatchNormalization())\n",
    "\n",
    "#MaxPooling - takes the max value of each 2x2 pool in the feature map\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "cnn_model.add(BatchNormalization())\n",
    "        \n",
    "#3rd convolution with 64 filters\n",
    "cnn_model.add(Conv2D(filters=64, kernel_size=(3,3),  padding = 'same', activation= 'relu'))\n",
    "cnn_model.add(BatchNormalization())\n",
    "        \n",
    "#4rd convolution with 128 filters\n",
    "cnn_model.add(Conv2D(filters=128, kernel_size=(3,3), padding = 'same', activation= 'relu'))\n",
    "cnn_model.add(BatchNormalization())\n",
    "              \n",
    "#Second MaxPool2D - to check with other options\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "cnn_model.add(BatchNormalization())\n",
    "        \n",
    "#5th convolutional layer with 24 3x3 filters - again, arbitrary\n",
    "cnn_model.add(Conv2D(filters=32, kernel_size=(4,4), padding = 'same', activation='relu'))\n",
    "cnn_model.add(BatchNormalization())\n",
    "    \n",
    "\n",
    "#second convolution with 36 filters\n",
    "cnn_model.add(Conv2D(filters=64, kernel_size=(3,3), padding = 'same', activation= 'relu'))\n",
    "cnn_model.add(BatchNormalization())\n",
    "        \n",
    "cnn_model.add(Conv2D(filters=128, kernel_size=(3,3), padding = 'same', activation= 'relu'))\n",
    "cnn_model.add(BatchNormalization())\n",
    "        \n",
    "#Second MaxPool2D - to check with other options\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "cnn_model.add(BatchNormalization())\n",
    "        \n",
    " #convolutional layer with 24 3x3 filters - again, arbitrary,\n",
    "cnn_model.add(Conv2D(filters=128, kernel_size=(4,4), padding = 'same', activation='relu'))\n",
    "cnn_model.add(BatchNormalization())\n",
    "        \n",
    "#second convolution with 36 filters\n",
    "cnn_model.add(Conv2D(filters=128, kernel_size=(3,3), padding = 'same', activation= 'relu'))\n",
    "cnn_model.add(BatchNormalization())\n",
    "        \n",
    "cnn_model.add(Conv2D(filters=128, kernel_size=(3,3), padding = 'same', activation= 'relu'))\n",
    "cnn_model.add(BatchNormalization())\n",
    "        \n",
    "#Second MaxPool2D - to check with other options\n",
    "cnn_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "cnn_model.add(BatchNormalization())\n",
    "        \n",
    "#the result of kthe CNN is then flattened and placed into the \n",
    "cnn_model.add(Flatten())\n",
    "        \n",
    "cnn_model.add(Dense(128, activation='relu'))\n",
    "cnn_model.add(BatchNormalization())\n",
    "\n",
    "#Add Dropout\n",
    "cnn_model.add(Dropout(0.4))\n",
    "cnn_model.add(Dense(256, activation='relu'))\n",
    "cnn_model.add(BatchNormalization())\n",
    "\n",
    "#final layer, is output, 1 out of 5 possible results\n",
    "#0 Normal, 1 Tapered, 2 Pyriform, 3 Amorphous\n",
    "cnn_model.add(Dense(4))\n",
    "              \n",
    "cnn_model.add(Activation('softmax'))\n",
    "              \n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnn_model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 27s 145ms/step - loss: 1.4384 - accuracy: 0.4216 - val_loss: 2.0145 - val_accuracy: 0.2188\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.21875, saving model to /Users/joanarafael/Documentos/NOVA_IMS/Data Science/2S_DataScience/WorkGroup/Deep Learning/CNN_SpermCells/Models/\n",
      "INFO:tensorflow:Assets written to: /Users/joanarafael/Documentos/NOVA_IMS/Data Science/2S_DataScience/WorkGroup/Deep Learning/CNN_SpermCells/Models/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/joanarafael/Documentos/NOVA_IMS/Data Science/2S_DataScience/WorkGroup/Deep Learning/CNN_SpermCells/Models/assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = cnn_model.fit(cnn_training,\n",
    "        epochs=200, \n",
    "        validation_data=cnn_val,\n",
    "        verbose = 1, \n",
    "        callbacks = [mc, reduce_lr, es,cb])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 12s 68ms/step - loss: 1.0373 - accuracy: 0.6140\n",
      "4/4 [==============================] - 0s 49ms/step - loss: 2.0145 - accuracy: 0.2188\n",
      "Train: 0.614, Val: 0.219\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD7CAYAAAB+B7/XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYbUlEQVR4nO3de3TU5b3v8c8MCaEUEAgT2EAb21rBC1RcYK0oFbYSIDcFUQJFV5EIqDtKLRoRGosrEC9FWumxsqVyqGhP8MJFBTWysXK0Vak7BgiFpaYaQsiFWyYmhGSe84fH7GJI5prJzDPv11oumHl+8+P7mSw+jE9+mXEYY4wAANZwdvUAAIDQotgBwDIUOwBYhmIHAMtQ7ABgGYodACxDsQOAZeK6egBJOnasXh5PdF1On5jYS7W17q4eI6xiLXOs5ZXIHC2cTof69ft2u+sRUewej4m6YpcUlTMHK9Yyx1peicw2YCsGACxDsQOAZSJiKwYAfGGM0bFj1WpqapQUmu2TqiqnPB5PSM4VWg51795D/fq55HA4/HokxQ4garjdJ+RwODRw4FA5HKHZcIiLc6q5OfKK3RiPjh+vkdt9Qr179/XrsWzFAIgaDQ1u9e7dN2SlHskcDqd69+6nhgb/r9ix/9kBYA2Pp0XdusXORkO3bnHyeFr8fhzFDiCq+LvfHM0CzepTsa9evVqpqalKTU3VI4880ma9tLRUU6dOVUpKih544AE1NzcHNAwARBO326377/+lz8fv379PBQUPdeJEX/Fa7O+++6527dqll19+WZs2bdLevXv15ptvnnHMokWL9Ktf/Uqvv/66jDEqLCzstIEBIFLU1Z3UwYP/8Pn44cMvVG7u0k6c6CteN6tcLpdyc3PVvXt3SdIPfvADVVRUtK4fOnRIjY2NuuSSSyRJU6dO1e9+9zvNnDmzcyYGgAixatWjqqmp1v33/1L//OdnOuecvkpISFB+/iNaseIhVVdXqaamWqNHX6bc3KX66KPd+uMf12j16jW6887bdOGFF6m4+L91/Pgx3X33Iv3kJ2NDMpfXYv/hD3/Y+vuysjJt27ZNzz//fOt9VVVVcrlcrbddLpeOHDni1xCJib38Oj5SuFy9u3qEsIu1zLGWV4rszFVVTsXF/c9Gw66PK/SX/67o4BGBG3fJYF05cnCHx9xzz326/fZsLVz4S02dmqaXXvq9Bg8erDfe2K5hw4apoOBRnT59WllZ0/TJJ/9Qt25OORwOxcV99WtLS7PWrv3feuedt/X000/qqquuavNnOJ1Ov78mPn97+eDBg5o3b57uvfdenXvuua33ezyeMzb4jTF+b/jX1rqj7r0aXK7eqq6u6+oxwirWMsdaXinyM3s8njOuOW9pMTJBVofDobOeo6XFeL2+vaXF0/prv379lZQ0SM3NHk2YMFH79u3Rc889q7Kyz3T8+AnV1dXLGCNjvjqvMUZjxvxEzc0eJSd/XydPnjzrn+fxeNp8TZxOR4cviH0q9t27dysnJ0eLFy9WamrqGWuDBg1SdXV16+2amholJSX5cloACMrYEf+msSP+LahzhOoHlBISElp//8ILf9bOnTuUkXG9brjhMn322ScyZ/nX4+stbofDcdb1QHn95unhw4d1xx136LHHHmtT6pI0ZMgQJSQkaPfu3ZKkzZs3a9y4cSEbEAAiVbdu3dTS0vY68w8++JsyMqZq4sTJampq0sGDB8L6tgVeX7GvXbtWp06dUkFBQet9M2bM0I4dO5STk6MRI0boscce05IlS+R2u3XRRRfp5ptv7tShASAS9O+fqIEDB2n58l+fcf+NN87UY4+t0LPPPqNvf7uXLr54pA4frtCQIUPDMpfDhPL1f4DYY48OsZY51vJKkZ+5svKfGjQoOaTnjNT3ivna2TJ722PnJ08BwDIUOwBYhmIHAMtQ7ABgGYodACxDsQOAZSh2ALAMxQ4AnSw//0G99trWsP15FDsAWCZ2PjwQgHVOH/i/Ov2PvwR1jvbegCt+2DjFn9/++6MvXrxIEydO0tVX/7skac6cn+k//mOh1qz5Xzp1qlF1dW7l5CzUVVddHdR8geAVOwAEICVlioqKXpckffHF52pqatKLL/4f5eYu1R//uEG5uUv0n//5ZJfMxit2AFEr/vyxHb6q9kWg7xVzxRVX6vHHH9GXX9arqOh1paRM1o03ztS7776j//qvIu3dW6KGhoagZgsUr9gBIADx8fEaO/Yq7dr1F+3Y8aauvXaS7rgjW6WlezVs2HDdfPOckL7Huj8odgAIUErKFP35z8/qnHP6qmfPnvrii3/q1lvn6/LLx+qdd94O63uw/yu2YgAgQCNHXiK3263rrrtBffqco7S0TM2efaPi4uJ06aVj1NjY2CXbMbwfe4Ai/X2rO0OsZY61vFLkZ+b92L8Skvdjd7vdSktLU3l5eZu1t99+W+np6UpPT9c999yj+vp6P8cGAISS12IvLi5WVlaWysrK2qydPHlSubm5evzxx7V161YNHz5cjz/+eGfMCQDwkddiLywsVF5enpKSktqslZWVafDgwTrvvPMkSePHj1dRUVHopwSA/y8Cdo/DJtCsXr95mp+f3+7aueeeq8rKSu3fv1/Dhw/Xtm3bVFNTE9AgAOCN09lNLS3NiouL7+pRwqKlpVlOZze/HxfUVTF9+vTRww8/rKVLl8rj8ejGG29UfLz/T3hH3wSIZC5X764eIexiLXOs5ZUiO7MxiaqrO6F+/QbI4Qjd1dpxcZF35bcxHp04cUIuV6LfX5Ogir2lpUWDBg3Sxo0bJUkff/yxvvOd7/h9Hq6KiQ6xljnW8krRkLmHmpvrdOjQ55JC0xlOp7PLrjfvmEPdu/eQ1KPN18TbVTFBFbvD4dCcOXO0ceNGJSUlad26dZoyZUowpwSAdjkcDvXv3/b7fcGI/H/M/BfQ/39kZ2erpKRETqdTy5Yt09y5czVp0iT16dNHt956a6hnBAD4gR9QCpCN/8p7E2uZYy2vROZoEZIfUAIARA+KHQAsQ7EDgGUodgCwDMUOAJah2AHAMhQ7AFiGYgcAy1DsAGAZih0ALEOxA4BlKHYAsAzFDgCWodgBwDIUOwBYhmIHAMv4VOxut1tpaWkqLy9vs7Z3715NmzZNGRkZmjdvnk6ePBnyIQEAvvNa7MXFxcrKylJZWdlZ1/Pz85WTk6MtW7boe9/7ntauXRvqGQEAfvBa7IWFhcrLy1NS0tk/QNbj8ai+vl6S1NDQoB49eoR2QgCAX+K8HZCfn9/hem5urubMmaPly5frW9/6lgoLC0M2HADAf16LvSONjY164IEHtG7dOo0cOVLPPPOM7rvvPq1Zs8av83T0oayRzOXq3dUjhF2sZY61vBKZbRBUsR84cEAJCQkaOXKkJOmmm27Sb3/7W7/PU1vrlsdjghkl7KLxk82DFWuZYy2vROZo4XQ6OnxBHNTljsnJyaqsrNSnn34qSXrrrbc0YsSIYE4JAAhSQK/Ys7OzlZOToxEjRmjFihW6++67ZYxRYmKili9fHuoZAQB+cBhjunwPhK2Y6BBrmWMtr0TmaNGpWzEAgMhDsQOAZSh2ALAMxQ4AlqHYAcAyFDsAWIZiBwDLUOwAYBmKHQAsQ7EDgGUodgCwDMUOAJah2AHAMhQ7AFiGYgcAy1DsAGAZih0ALOPTR+O53W7NmDFDf/jDHzR06NDW+0tLS5Wbm9t6++jRozrnnHP0yiuvhH5SAIBPvBZ7cXGxlixZorKysjZrF1xwgTZv3ixJamho0PTp0/Xggw+GekYAgB+8bsUUFhYqLy9PSUlJHR731FNPacyYMRo9enTIhgMA+M/rK/b8/HyvJ6mrq1NhYaG2bt0akqEAAIHzaY/dmy1btuiaa65RYmJiQI/v6NO2I5nL1burRwi7WMsca3klMtsgJMVeVFSkefPmBfz42lq3PB4TilHCxuXqrerquq4eI6xiLXOs5ZXIHC2cTkeHL4iDvtzRGKO9e/dq1KhRwZ4KABACARV7dna2SkpKJH11iWN8fLwSEhJCOhgAIDAOY0yX74GwFRMdYi1zrOWVyBwtOn0rBgAQWSh2ALAMxQ4AlqHYAcAyFDsAWIZiBwDLUOwAYBmKHQAsQ7EDgGUodgCwDMUOAJah2AHAMhQ7AFiGYgcAy1DsAGAZih0ALONTsbvdbqWlpam8vLzN2qeffqrZs2crIyNDt956q06cOBHyIQEAvvNa7MXFxcrKylJZWVmbNWOMFixYoOzsbG3ZskUXXHCB1qxZ0xlzAgB85LXYCwsLlZeXp6SkpDZre/fuVc+ePTVu3DhJ0vz58zVr1qzQTwkA8FmctwPy8/PbXfv88881YMAALV68WKWlpfr+97+vpUuXhnRAAIB/vBZ7R5qbm/X+++/r2Wef1YgRI7Rq1SoVFBSooKDAr/N09KGskczl6t3VI4RdrGWOtbwSmW0QVLG7XC4lJydrxIgRkqS0tDTl5OT4fZ7aWrc8HhPMKGEXjZ9sHqxYyxxreSUyRwun09HhC+KgLnccNWqUjh49qv3790uSduzYoYsuuiiYUwIAghRQsWdnZ6ukpEQ9evTQ73//ey1ZskSpqan629/+ptzc3FDPCADwg8MY0+V7IGzFRIdYyxxreSUyR4tO3YoBAEQeih0ALEOxA4BlKHYAsAzFDgCWodgBwDIUOwBYhmIHAMtQ7ABgGYodACxDsQOAZSh2ALAMxQ4AlqHYAcAyFDsAWIZiBwDLUOwAYBmfit3tdistLU3l5eVt1lavXq3x48crMzNTmZmZ2rBhQ8iHBAD4Ls7bAcXFxVqyZInKysrOur5nzx6tXLlSo0aNCvVsAIAAeH3FXlhYqLy8PCUlJZ11fc+ePXrqqaeUnp6uZcuW6dSpUyEfEgDgO58/zHrChAlav369hg4d2npffX297r77buXm5io5OVm5ubkaMmSIFi5c2GkDAwA6FlSxf9O+ffu0ePFibdq0ya8hamvd8nh8GiNiROMnmwcr1jLHWl6JzNHC6XQoMbFX++vBnLyiokIvvPBC621jjOLivG7bAwA6UVDF3qNHDz366KP64osvZIzRhg0bdO2114ZqNgBAAAIq9uzsbJWUlKh///5atmyZFixYoEmTJskYo5///OehnhEA4Aef99g7E3vs0SHWMsdaXonM0aJT99gBAJGHYgcAy1DsAGAZih0ALEOxA4BlKHYAsAzFDgCWodgBwDIUOwBYhmIHAMtQ7ABgGYodACxDsQOAZSh2ALAMxQ4AlqHYAcAyPhW72+1WWlqaysvL2z1m586dmjBhQsgGAwAExmuxFxcXKysrS2VlZe0eU1NTo4cffjiUcwEAAuS12AsLC5WXl6ekpKR2j1myZInuvPPOkA4GAAhMnLcD8vPzO1xfv369LrzwQv3oRz8K2VAAgMB5LfaOHDhwQG+88YbWrVunysrKgM/T0YeyRjKXq3dXjxB2sZY51vJKZLZBUMW+fft2VVdXa9q0aTp9+rSqqqo0c+ZMPffcc36dp7bWLY/HBDNK2EXjJ5sHK9Yyx1peiczRwul0dPiCOKhiz8nJUU5OjiSpvLxcN998s9+lDgAIrYCuY8/OzlZJSUmoZwEAhIDDGNPleyBsxUSHWMsca3klMkcLb1sx/OQpAFiGYgcAy1DsAGAZih0ALEOxA4BlKHYAsAzFDgCWodgBwDIUOwBYhmIHAMtQ7ABgGYodACxDsQOAZSh2ALAMxQ4AlqHYAcAyPhW72+1WWlqaysvL26y9+eabSk9PV2pqqnJzc9XU1BTyIQEAvvNa7MXFxcrKylJZWVmbtS+//FLLli3TM888o1dffVWnTp3Syy+/3BlzAgB85LXYCwsLlZeXp6SkpDZrPXv21I4dOzRgwAA1NDSotrZWffr06ZRBAQC+8Vrs+fn5Gj16dLvr8fHxevvtt3X11Vfr2LFjuvLKK0M6IADAPz5/mPWECRO0fv16DR06tN1jVq5cqUOHDuk3v/lNyAYEAPgnLpgHHz9+XHv27Gl9lZ6enq6FCxf6fZ7aWrc8Hp/+fYkY0fjJ5sGKtcyxllcic7RwOh1KTOzV/nowJzfGaNGiRaqoqJAkbd++XZdeemkwpwQABCmgYs/OzlZJSYn69eunhx56SPPmzVNGRoY+++wzLVq0KNQzAgD84PMee2diKyY6xFrmWMsrkTladOpWDAAg8lDsAGAZih0ALEOxA4BlKHYAsAzFDgCWodgBwDIUOwBYhmIHAMtQ7ABgGYodACxDsQOAZSh2ALAMxQ4AlqHYAcAyFDsAWIZiBwDL+FTsbrdbaWlpKi8vb7NWVFSkzMxMZWRk6Pbbb9eJEydCPiQAwHdei724uFhZWVkqKytrs+Z2u/Xggw9qzZo12rJli4YNG6YnnniiM+YEAPjIa7EXFhYqLy9PSUlJbdZOnz6tvLw8DRw4UJI0bNgwHT58OPRTAgB85vOHWU+YMEHr16/X0KFDz7re2NiomTNnavbs2br++utDOiQAwHch+eZpXV2dbrvtNg0fPpxSB4AuFnSxV1VVaebMmRo2bJjy8/NDMRMAIAhxwTy4paVF8+fP1+TJk3X77beHaiYAQBACKvbs7Gzl5OSosrJS+/btU0tLi15//XVJ0sUXX8wrdwDoQj5/8xQAEB34yVMAsAzFDgCWodgBwDIUOwBYhmIHAMtQ7O2oqKjQrFmzNGnSJC1YsED19fVtjmlqatKiRYs0efJkXX/99frkk0/OWG9ubtZNN92kl156KVxjByWYzPX19brrrruUnp6u9PR0vfrqq+Ee3y9bt27VlClTNHHiRG3YsKHNemlpqaZOnaqUlBQ98MADam5uluTbcxSpAs28e/du3XDDDcrMzNQtt9yiQ4cOhXv0gAWa+Wv79u3TxRdfHK5xQ8fgrG677TbzyiuvGGOMWb16tXnkkUfaHPP000+bpUuXGmOMef/998306dPPWF+1apW57LLLzIsvvtj5A4dAMJlXrlxpCgoKjDHG1NTUmLFjx5rq6uowTe6fyspKM378eHPs2DFTX19v0tPTzcGDB884JjU11Xz00UfGGGPuv/9+s2HDBmOMb89RJAom8/jx401paakxxpiNGzea+fPnh3X2QAWT2RhjvvzySzNjxgxz/vnnh3PskOAV+1mcPn1aH3zwgVJSUiRJU6dO1fbt29sct3PnTmVkZEiSxowZo6NHj6qiokKS9Pe//1379+/X+PHjwzd4EILNfNlll2n27NmSpMTERPXt21c1NTXhC+CHd999V5dffrn69u2rnj17KiUl5Yyshw4dUmNjoy655BJJ//Nc+PocRaJAMzc1Nemuu+7S8OHDJUXXO7gGmvlrBQUFuuWWW8I9dkhQ7Gdx7Ngx9erVS3FxX/1grsvl0pEjR9ocV1VVJZfL1Xrb5XKpsrJSbrdbK1as0EMPPRS2mYMVbOaxY8dq8ODBkqTXXntNTU1NOu+888IzvJ++mSEpKemMrGfLeOTIEZ+fo0gUaObu3bsrMzNTkuTxeLR69Wpdc8014Rs8CIFmlqS33npLjY2NmjRpUvgGDqGg3ivGBtu2bdOKFSvOuC85OVkOh+OM+755W5KMMWfcb4yR0+nUr3/9a82bN08DBgzonKGD1BmZ//Xcy5cv19NPP91agJHG4/G0yfCvt9tb/+Zx0tmfo0gUaOavNTU1KTc3V83NzZo3b154hg5SoJmrq6v15JNPat26deEcN6Qi829eGE2ePFmTJ08+477Tp0/rxz/+sVpaWtStWzdVV1ef9YNGBg4cqKqqKn33u9+VJNXU1Mjlcum9997TgQMH9MQTT+jw4cP661//qri4uNYtjK4W6sxfH/enP/1Ja9eu1dq1azVs2LDODxKgQYMG6cMPP2y9/c2sgwYNUnV1devtrzP2799fdXV1Xp+jSBRoZumrb4wvWLBAffv21ZNPPqn4+PjwDR6EQDPv3LlTx48f16xZs1rXMjMztWHDBvXq1Ss8wweJrZiziI+P1+jRo/Xaa69JkjZt2qRx48a1Oe6nP/2pNm/eLEn68MMPlZCQoCFDhmjXrl3avHmzNm/erAkTJignJydiSr09wWQePHiwioqKtG7dOj3//PMRXeqSdMUVV+i9997T0aNH1dDQoDfeeOOMrEOGDFFCQoJ2794tSdq8ebPGjRvn83MUiQLNLEmLFi1ScnKyVq1ape7du3fJ/IEINPP06dNVVFTU+nf467VoKXVJXBXTnvLycvOzn/3MTJ482cyZM8ccP37cGGPMc889Z1atWmWMMaaxsdHce++9ZsqUKea6664ze/bsaXOe++67L2quigkmc3p6uhk7dqzJyMho/e/jjz/usizebNmyxaSmppqJEyeaNWvWGGOMmTt3buvMpaWlZtq0aSYlJcX84he/MKdOnTLGtP8cRYNAMu/du9ecf/75ZsqUKa1f17lz53ZlDL8E+nX+V9F4VQzv7ggAlmErBgAsQ7EDgGUodgCwDMUOAJah2AHAMhQ7AFiGYgcAy1DsAGCZ/wfTSr2W2cgjuwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "_, train_acc = cnn_model.evaluate(cnn_training, verbose=1)\n",
    "_, val_acc = cnn_model.evaluate(cnn_val, verbose=1)\n",
    "print('Train: %.3f, Val: %.3f' % (train_acc, val_acc))\n",
    "\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "#get predicted probality\n",
    "prediction = cnn_model.predict(cnn_test,verbose=1)\n",
    "\n",
    "#Get class of prediction\n",
    "predicted_class = np.argmax(prediction,axis=1)\n",
    "\n",
    "#get trueclass\n",
    "true_classes = cnn_test.classes\n",
    "\n",
    "# get names of pictures\n",
    "filenames = cnn_test.filenames\n",
    "\n",
    "\n",
    "#store info in dataframe\n",
    "df_predictions = pd.DataFrame({'Filename': filenames, 'Label': true_classes, 'Test': predicted_class})\n",
    "\n",
    "#store info in dataframe\n",
    "cnn_model_predictions = pd.DataFrame({'Filename': filenames,'cnn_model': predicted_class})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7  Confusion matrix and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0  31   0   0]\n",
      " [  0  57   0   0]\n",
      " [  0  27   0   0]\n",
      " [  0 142   0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      class0       0.00      0.00      0.00        31\n",
      "      class1       0.22      1.00      0.36        57\n",
      "      class2       0.00      0.00      0.00        27\n",
      "      class3       0.00      0.00      0.00       142\n",
      "\n",
      "    accuracy                           0.22       257\n",
      "   macro avg       0.06      0.25      0.09       257\n",
      "weighted avg       0.05      0.22      0.08       257\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "class_labels = list(cnn_test.class_indices.keys())   \n",
    "\n",
    "print(confusion_matrix(true_classes, predicted_class))\n",
    "\n",
    "\n",
    "report = classification_report(true_classes, predicted_class, target_names=class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a> \n",
    "\n",
    "# 4. ALEX NET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = path+'/Models/AlexNet.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Preprocess input\n",
    "Online Data Augmentation  in RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11217 images belonging to 4 classes.\n",
      "Found 256 images belonging to 4 classes.\n",
      "Found 257 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Alex_train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   rotation_range = 5,\n",
    "                                   width_shift_range = 0.1,\n",
    "                                   height_shift_range = 0.1, \n",
    "                                   vertical_flip = True,\n",
    "                                   horizontal_flip = True,\n",
    "                                   brightness_range=[0.2,1.5], \n",
    "                                   fill_mode='nearest',\n",
    "                                   zoom_range = 0.2,\n",
    "                                   ) \n",
    "\n",
    "Alex_val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "Alex_test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "#test different color maps -  class modes and cross validation types\n",
    "Alex_training = Alex_train_datagen.flow_from_directory(path+'/path/train',\n",
    "                                                 target_size = (32, 32),\n",
    "                                                 batch_size = 64,\n",
    "                                                 shuffle = True,\n",
    "                                                 class_mode=\"categorical\",\n",
    "                                                 color_mode = 'rgb')\n",
    "\n",
    "Alex_val = Alex_val_datagen.flow_from_directory(path+'/path/val',\n",
    "                                            target_size = (32, 32),\n",
    "                                            batch_size = 64,\n",
    "                                            shuffle = True,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            color_mode = 'rgb')\n",
    "\n",
    "Alex_test = Alex_test_datagen.flow_from_directory(path+'/path/test',\n",
    "                                            target_size = (32, 32),\n",
    "                                            batch_size = 1,\n",
    "                                            shuffle = True,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            color_mode = 'rgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Build model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 8, 8, 96)          34944     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 8, 8, 96)          384       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 8, 8, 96)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 96)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 256)         614656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 2, 2, 384)         885120    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 2, 2, 384)         1536      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2, 2, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 2, 2, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 2, 2, 384)         1536      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2, 2, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 256)         884992    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 2, 2, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4096)              1052672   \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 4004      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 4)                 16        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 25,724,476\n",
      "Trainable params: 25,703,332\n",
      "Non-trainable params: 21,144\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Instantiation\n",
    "AlexNet = Sequential()\n",
    "\n",
    "#1st Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=96, input_shape=(32,32,3), kernel_size=(11,11), strides=(4,4), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#2nd Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#3rd Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "#4th Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "#5th Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#Passing it to a Fully Connected layer\n",
    "AlexNet.add(Flatten())\n",
    "\n",
    "# 1st Fully Connected Layer\n",
    "AlexNet.add(Dense(4096, input_shape=(32,32,3,)))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "# Add Dropout to prevent overfitting\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "#2nd Fully Connected Layer\n",
    "AlexNet.add(Dense(4096))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "#Add Dropout\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "#3rd Fully Connected Layer\n",
    "AlexNet.add(Dense(1000))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "#Add Dropout\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "#Output Layer\n",
    "AlexNet.add(Dense(4))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('softmax'))\n",
    "\n",
    "#Model Summary\n",
    "AlexNet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 51s 279ms/step - loss: 1.3696 - accuracy: 0.3990 - val_loss: 1.6569 - val_accuracy: 0.1211\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.21875\n"
     ]
    }
   ],
   "source": [
    "history = AlexNet.fit(Alex_training,\n",
    "        epochs=200, \n",
    "        validation_data=Alex_val,\n",
    "        verbose = 1, \n",
    "        callbacks = [mc, reduce_lr, es,cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Evaluate  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 13s 72ms/step - loss: 1.6717 - accuracy: 0.2423\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 1.6569 - accuracy: 0.1211\n",
      "Train: 0.242, Val: 0.121\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, train_acc = AlexNet.evaluate(Alex_training, verbose=1)\n",
    "_, val_acc = AlexNet.evaluate(Alex_val, verbose=1)\n",
    "print('Train: %.3f, Val: %.3f' % (train_acc, val_acc))\n",
    "\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 2s 7ms/step - loss: 1.6697 - accuracy: 0.1206\n",
      "Test loss: 1.6696956157684326\n",
      "Test accuracy: 0.12062256783246994\n",
      "\n",
      "Time: 1.3651794827000003 min\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = AlexNet.evaluate(Alex_test)\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)\n",
    "print(\"\\nTime:\",sum(cb.logs)/60,\"min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6 Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 2s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "#get predicted probality\n",
    "prediction = AlexNet.predict(Alex_test,verbose=1)\n",
    "\n",
    "#Get class of prediction\n",
    "predicted_class = np.argmax(prediction,axis=1)\n",
    "\n",
    "#get trueclass\n",
    "true_classes = Alex_test.classes\n",
    "\n",
    "# get names of pictures\n",
    "filenames = Alex_test.filenames\n",
    "\n",
    "#store info in dataframe\n",
    "alex_predictions = pd.DataFrame({'Filename': filenames,'AlexNet': predicted_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join prediction to dataframe\n",
    "df_predictions = df_predictions.merge(alex_predictions, on = 'Filename', how = 'inner')\n",
    "\n",
    "del alex_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Label</th>\n",
       "      <th>Test</th>\n",
       "      <th>AlexNet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>class0/ch00_p1-pl2-sample10-sperm4.tif</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>class0/ch00_p1-pl2-sample12-sperm13.tif</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>class0/ch00_p1-pl2-sample14-sperm9.tif</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>class0/ch00_p1-pl2-sample19-sperm7.tif</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>class0/ch00_p1-pl2-sample3-sperm12.tif</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>class3/image_006.BMP</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>class3/image_007.BMP</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>class3/image_008.BMP</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>class3/image_012.BMP</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>class3/image_041.BMP</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Filename  Label  Test  AlexNet\n",
       "0     class0/ch00_p1-pl2-sample10-sperm4.tif      0     1        0\n",
       "1    class0/ch00_p1-pl2-sample12-sperm13.tif      0     1        0\n",
       "2     class0/ch00_p1-pl2-sample14-sperm9.tif      0     1        0\n",
       "3     class0/ch00_p1-pl2-sample19-sperm7.tif      0     1        0\n",
       "4     class0/ch00_p1-pl2-sample3-sperm12.tif      0     1        0\n",
       "..                                       ...    ...   ...      ...\n",
       "252                     class3/image_006.BMP      3     1        0\n",
       "253                     class3/image_007.BMP      3     1        0\n",
       "254                     class3/image_008.BMP      3     1        0\n",
       "255                     class3/image_012.BMP      3     1        0\n",
       "256                     class3/image_041.BMP      3     1        0\n",
       "\n",
       "[257 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7  Confusion matrix and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 31   0   0   0]\n",
      " [ 57   0   0   0]\n",
      " [ 27   0   0   0]\n",
      " [142   0   0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      class0       0.12      1.00      0.22        31\n",
      "      class1       0.00      0.00      0.00        57\n",
      "      class2       0.00      0.00      0.00        27\n",
      "      class3       0.00      0.00      0.00       142\n",
      "\n",
      "    accuracy                           0.12       257\n",
      "   macro avg       0.03      0.25      0.05       257\n",
      "weighted avg       0.01      0.12      0.03       257\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_labels = list(Alex_test.class_indices.keys())   \n",
    "\n",
    "print(confusion_matrix(true_classes, predicted_class))\n",
    "\n",
    "report = classification_report(true_classes, predicted_class, target_names=class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a> \n",
    "# 5. RESNET50 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = path+'/Models/ResNet50.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Preprocess input\n",
    "resnet50 requires its own preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11217 images belonging to 4 classes.\n",
      "Found 256 images belonging to 4 classes.\n",
      "Found 257 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#to play around with these\n",
    "res_train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   preprocessing_function=preprocess_input,\n",
    "                                   rotation_range = 5,\n",
    "                                   width_shift_range = 0.06, \n",
    "                                   height_shift_range = 0.06, \n",
    "                                   vertical_flip = True,\n",
    "                                   horizontal_flip = True,\n",
    "                                   brightness_range=[0.2,1.2], \n",
    "                                   fill_mode='nearest',\n",
    "                                   zoom_range = 0.2,\n",
    "                                   ) \n",
    "\n",
    "res_val_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function=preprocess_input)\n",
    "res_test_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function=preprocess_input)\n",
    "\n",
    "#test different color maps -  class modes and cross validation types\n",
    "res_train = res_train_datagen.flow_from_directory(path+'/path/train',\n",
    "                                                 target_size = (32, 32),\n",
    "                                                 batch_size = 64,\n",
    "                                                 class_mode=\"categorical\",\n",
    "                                                 color_mode = 'rgb',\n",
    "                                                 shuffle = True)\n",
    "\n",
    "res_val = res_val_datagen.flow_from_directory(path+'/path/val',\n",
    "                                            target_size = (32, 32),\n",
    "                                            batch_size = 64,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            color_mode = 'rgb',\n",
    "                                            shuffle = True)\n",
    "\n",
    "res_test = res_test_datagen.flow_from_directory(path+'/path/test',\n",
    "                                            target_size = (32, 32),\n",
    "                                            batch_size = 1,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            color_mode = 'rgb',\n",
    "                                            shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Build model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Functional)        (None, 1, 1, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4096)              8392704   \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 48,810,884\n",
      "Trainable params: 25,206,788\n",
      "Non-trainable params: 23,604,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "res50model = Sequential()\n",
    "\n",
    "#Layer 1: RES50 without top layer\n",
    "res50model.add(ResNet50(weights='imagenet', input_shape= (32,32,3),\n",
    "                 include_top = False, classes=5,))\n",
    "\n",
    "#Passing it to a Fully Connected layer\n",
    "res50model.add(Flatten())\n",
    "# 1st Fully Connected Layer\n",
    "res50model.add(Dense(4096, input_shape=(32,32,3,)))\n",
    "res50model.add(BatchNormalization())\n",
    "res50model.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "res50model.add(Dropout(0.4))\n",
    "\n",
    "#2nd Fully Connected Layer\n",
    "res50model.add(Dense(4096))\n",
    "res50model.add(BatchNormalization())\n",
    "res50model.add(Activation('relu'))\n",
    "\n",
    "#Add Dropout\n",
    "res50model.add(Dropout(0.4))\n",
    "\n",
    "res50model.add(Dense(4, activation = 'softmax'))\n",
    "\n",
    "#freeze layers in resnet - weights obtained with IMAGENET challenge, we only train final layer\n",
    "res50model.layers[0].trainable = False\n",
    "\n",
    "res50model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50model.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 29s 154ms/step - loss: 1.6878 - accuracy: 0.5380 - val_loss: 10.4327 - val_accuracy: 0.2188\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.21875\n"
     ]
    }
   ],
   "source": [
    "history = res50model.fit(res_train,\n",
    "        epochs=200, \n",
    "        validation_data=res_val,\n",
    "        verbose = 1, \n",
    "        callbacks = [mc, reduce_lr, es,cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 19s 105ms/step - loss: 5.0203 - accuracy: 0.6140\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 10.4327 - accuracy: 0.2188\n",
      "Train: 0.614, Val: 0.219\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 4s 15ms/step - loss: 10.4947 - accuracy: 0.2218\n",
      "Test loss: 10.494688034057617\n",
      "Test accuracy: 0.2217898815870285\n",
      "\n",
      "Time: 2.628499286733336 min\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_, train_acc = res50model.evaluate(res_train, verbose=1)\n",
    "_, val_acc = res50model.evaluate(res_val, verbose=1)\n",
    "print('Train: %.3f, Val: %.3f' % (train_acc, val_acc))\n",
    "\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = res50model.evaluate(res_test)\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "print(\"\\nTime:\",sum(cb.logs)/60,\"min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6 Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 4s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "#get predicted probality\n",
    "prediction = res50model.predict(res_test,verbose=1)\n",
    "\n",
    "#Get class of prediction\n",
    "predicted_class = np.argmax(prediction,axis=1)\n",
    "\n",
    "#get trueclass\n",
    "true_classes = res_test.classes\n",
    "\n",
    "# get names of pictures\n",
    "filenames = res_test.filenames\n",
    "\n",
    "\n",
    "#store info in dataframe\n",
    "df_predictions = pd.DataFrame({'Filename': filenames, 'Label': true_classes, 'Test': predicted_class})\n",
    "\n",
    "#store info in dataframe\n",
    "res50_prebuilt_predictions = pd.DataFrame({'Filename': filenames,'ResNet50': predicted_class})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.7  Confusion matrix and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0  31   0   0]\n",
      " [  0  57   0   0]\n",
      " [  0  27   0   0]\n",
      " [  0 142   0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      class0       0.00      0.00      0.00        31\n",
      "      class1       0.22      1.00      0.36        57\n",
      "      class2       0.00      0.00      0.00        27\n",
      "      class3       0.00      0.00      0.00       142\n",
      "\n",
      "    accuracy                           0.22       257\n",
      "   macro avg       0.06      0.25      0.09       257\n",
      "weighted avg       0.05      0.22      0.08       257\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_labels = list(res_test.class_indices.keys())   \n",
    "\n",
    "print(confusion_matrix(true_classes, predicted_class))\n",
    "\n",
    "report = classification_report(true_classes, predicted_class, target_names=class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a> \n",
    "\n",
    "# 6. EFFICIENT NET B7\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = path+'/Models/EffnetB7.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Preprocess input\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11217 images belonging to 4 classes.\n",
      "Found 256 images belonging to 4 classes.\n",
      "Found 257 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "#to play around with these\n",
    "EffNetB7train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   rotation_range = 5,\n",
    "                                   width_shift_range = 0.06, \n",
    "                                   height_shift_range = 0.06, \n",
    "                                   vertical_flip = True,\n",
    "                                   horizontal_flip = True,\n",
    "                                   brightness_range=[0.2,1.2], \n",
    "                                   fill_mode='nearest',\n",
    "                                   zoom_range = 0.2,\n",
    "                                   ) \n",
    "\n",
    "EffNetB7val_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function=preprocess_input)\n",
    "EffNetB7test_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function=preprocess_input)\n",
    "\n",
    "#test different color maps -  class modes and cross validation types\n",
    "EffNetB7_train = EffNetB7train_datagen.flow_from_directory(path+'/path/train',\n",
    "                                                 target_size = (32, 32),\n",
    "                                                 batch_size = 64,\n",
    "                                                 class_mode=\"categorical\",\n",
    "                                                 color_mode = 'rgb',\n",
    "                                                 shuffle = True)\n",
    "\n",
    "EffNetB7_val = EffNetB7val_datagen.flow_from_directory(path+'/path/val',\n",
    "                                            target_size = (32, 32),\n",
    "                                            batch_size = 64,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            color_mode = 'rgb',\n",
    "                                            shuffle = True)\n",
    "\n",
    "EffNetB7_test = EffNetB7test_datagen.flow_from_directory(path+'/path/test',\n",
    "                                            target_size = (32, 32),\n",
    "                                            batch_size = 1,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            color_mode = 'rgb',\n",
    "                                            shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.2\n",
    "\n",
    "EffNetB7model = Sequential()\n",
    "\n",
    "#Layer 1\n",
    "\n",
    "EffNetB7model.add(EfficientNetB7(weights='imagenet', input_shape= (32,32,3),\n",
    "                      include_top = False, classes=4))\n",
    "EffNetB7model.add(layers.GlobalMaxPooling2D(name=\"gap\")) ### mais eficiente qie o flatten\n",
    "#EffNetB7.add(Flatten())\n",
    "if dropout_rate > 0:\n",
    "    EffNetB7model.add(Dropout(dropout_rate, name=\"dropout_out\"))\n",
    "\n",
    "EffNetB7model.add(Dense(4, activation=\"softmax\"))\n",
    "\n",
    "EffNetB7model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnetb7 (Functional)  (None, 1, 1, 2560)        64097687  \n",
      "_________________________________________________________________\n",
      "gap (GlobalMaxPooling2D)     (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dropout_out (Dropout)        (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 10244     \n",
      "=================================================================\n",
      "Total params: 64,107,931\n",
      "Trainable params: 10,244\n",
      "Non-trainable params: 64,097,687\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EffNetB7model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "EffNetB7model.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 47s 198ms/step - loss: 2.5272 - accuracy: 0.4648 - val_loss: 2.1559 - val_accuracy: 0.1211\n"
     ]
    }
   ],
   "source": [
    "EffNet_history = EffNetB7model.fit(EffNetB7_train,\n",
    "        epochs=200,\n",
    "        validation_data=EffNetB7_val,\n",
    "        verbose = 1, \n",
    "        validation_steps = len(EffNetB7_val),\n",
    "        callbacks = [reduce_lr,cb,es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 28s 161ms/step - loss: 1.2382 - accuracy: 0.2379\n",
      "4/4 [==============================] - 1s 154ms/step - loss: 2.1559 - accuracy: 0.1211\n",
      "Train: 0.238, Val: 0.121\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "_, train_acc = EffNetB7model.evaluate(EffNetB7_train, verbose=1)\n",
    "_, val_acc = EffNetB7model.evaluate(EffNetB7_val, verbose=1)\n",
    "print('Train: %.3f, Val: %.3f' % (train_acc, val_acc))\n",
    "\n",
    "# plot training history\n",
    "plt.plot(EffNet_history.history['loss'], label='train')\n",
    "plt.plot(EffNet_history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 7s 25ms/step - loss: 2.1510 - accuracy: 0.1206\n",
      "Test loss: 2.150968313217163\n",
      "Test accuracy: 0.12062256783246994\n",
      "\n",
      "Time: 2.628499286733336 min\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = EffNetB7model.evaluate(EffNetB7_test)\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)\n",
    "print(\"\\nTime:\",sum(cb.logs)/60,\"min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.5 Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 6s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "#get predicted probality\n",
    "prediction = EffNetB7model.predict(EffNetB7_test,verbose=1)\n",
    "\n",
    "#Get class of prediction\n",
    "predicted_class = np.argmax(prediction,axis=1)\n",
    "\n",
    "#get trueclass\n",
    "true_classes = EffNetB7_test.classes\n",
    "\n",
    "# get names of pictures\n",
    "filenames = EffNetB7_test.filenames\n",
    "\n",
    "#store info in dataframe\n",
    "EffNetB7_predictions = pd.DataFrame({'Filename': filenames,'EffNetB7': predicted_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join prediction to dataframe\n",
    "df_predictions = df_predictions.merge(EffNetB7_predictions, on = 'Filename', how = 'inner')\n",
    "\n",
    "del EffNetB7_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Label</th>\n",
       "      <th>Test</th>\n",
       "      <th>EffNetB7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>class0/ch00_p1-pl2-sample10-sperm4.tif</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>class0/ch00_p1-pl2-sample12-sperm13.tif</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>class0/ch00_p1-pl2-sample14-sperm9.tif</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>class0/ch00_p1-pl2-sample19-sperm7.tif</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>class0/ch00_p1-pl2-sample3-sperm12.tif</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>class3/image_006.BMP</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>class3/image_007.BMP</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>class3/image_008.BMP</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>class3/image_012.BMP</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>class3/image_041.BMP</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Filename  Label  Test  EffNetB7\n",
       "0     class0/ch00_p1-pl2-sample10-sperm4.tif      0     1         0\n",
       "1    class0/ch00_p1-pl2-sample12-sperm13.tif      0     1         0\n",
       "2     class0/ch00_p1-pl2-sample14-sperm9.tif      0     1         0\n",
       "3     class0/ch00_p1-pl2-sample19-sperm7.tif      0     1         0\n",
       "4     class0/ch00_p1-pl2-sample3-sperm12.tif      0     1         0\n",
       "..                                       ...    ...   ...       ...\n",
       "252                     class3/image_006.BMP      3     1         0\n",
       "253                     class3/image_007.BMP      3     1         0\n",
       "254                     class3/image_008.BMP      3     1         0\n",
       "255                     class3/image_012.BMP      3     1         0\n",
       "256                     class3/image_041.BMP      3     1         0\n",
       "\n",
       "[257 rows x 4 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.6 Confusion matrix and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 31   0   0   0]\n",
      " [ 57   0   0   0]\n",
      " [ 27   0   0   0]\n",
      " [142   0   0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      class0       0.12      1.00      0.22        31\n",
      "      class1       0.00      0.00      0.00        57\n",
      "      class2       0.00      0.00      0.00        27\n",
      "      class3       0.00      0.00      0.00       142\n",
      "\n",
      "    accuracy                           0.12       257\n",
      "   macro avg       0.03      0.25      0.05       257\n",
      "weighted avg       0.01      0.12      0.03       257\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_labels = list(EffNetB7_test.class_indices.keys())   \n",
    "\n",
    "print(confusion_matrix(true_classes, predicted_class))\n",
    "\n",
    "\n",
    "report = classification_report(true_classes, predicted_class, target_names=class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
