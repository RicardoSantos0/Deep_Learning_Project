{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project\n",
    "\n",
    "The current project relies on using CNNs in order to process sperm data. The goal is to develop a classifier able to identify sperm cells.\n",
    "\n",
    "The model is not yet fuly defined. We will start with a standard CNN to Dense Layer.\n",
    "\n",
    "The goal is for images to be loaded into the CNN. The CNN will will then perform feature extraction and those will be fed to the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Create the pipeline\n",
    "\n",
    "The Deep learning model will be made out of 2 different parts: \n",
    "\n",
    "1. A CNN that takes the images as inputs and performs feature extraction.\n",
    "2. A dense, fully connected layer that will perform classification itself with the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need this to run, don't know why\n",
    "import os\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "#Check Computer's available devices\n",
    "#Will need to check in the future\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "#cfg 1\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "#cfg2\n",
    "config = tf.compat.v1.ConfigProto(gpu_options = \n",
    "                         tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "# device_count = {'GPU': 1}\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import some libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "#image manipulation packages\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "#Data Augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#Classification\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix\n",
    "\n",
    "sns.set()\n",
    "#import keras\n",
    "from tensorflow import keras\n",
    "\n",
    "import glob\n",
    "import cv2\n",
    "import shutil\n",
    "import random\n",
    "import Augmentor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Label images\n",
    "\n",
    "In this case, we have a folder with 1132 image - SCIAN-MorphoSpermGS folder - https://cimt.uchile.cl/gold10/. Each image is 35 x 35 pixels and has been classified by 3 experts. We will use majority vote result as target. Each image will need to be loaded and the dataset will need to be created.\n",
    "\n",
    "#### This will yield a dataset with the picture name and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the path where you've put your dataset provided in Moodle\n",
    "\n",
    "#step 1: change directory back\n",
    "os.chdir('../Dataset')\n",
    "path = os.getcwd()\n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 - load txt file\n",
    "dataframe = pd.read_csv(path + '\\PA-expert-annotations.txt', sep = '\\\\t', header = None, engine = 'python')\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Start by most standard preprocessing of Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#editions to target dataset\n",
    "dataframe.rename(columns={0 :\"Sperm_Pic\", 1: 'Expert_1', 2: 'Expert_2', 3: 'Expert_3', 4: 'Majority_Vote'}, inplace = True)\n",
    "dataframe['Sperm_Pic'] = dataframe['Sperm_Pic'].str.replace('/S', '-s')\n",
    "dataframe['Majority_Vote'] = dataframe['Majority_Vote'].replace(5, 4)\n",
    "\n",
    "\n",
    "#add ch_00 to all rows:\n",
    "dataframe['Sperm_Pic'] = 'ch00_' + dataframe['Sperm_Pic'] + '.tif'\n",
    "\n",
    "#Drop all irrelevant features\n",
    "dataframe = dataframe.drop(['Expert_1', 'Expert_2', 'Expert_3'], axis = 1)\n",
    "\n",
    "#Show\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Place images on Folder based on image Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Partial-Agreement-Images')\n",
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imgs = []\n",
    "\n",
    "for image in glob.glob(path+'/*.tif'):\n",
    "        \n",
    "    imgs.append(cv2.imread(image))\n",
    "    \n",
    "    #names of each sample in the image and dataframe are different, gotta deal with this here to match them\n",
    "\n",
    "    img_name = image.split('\\\\')[-1]\n",
    "    check_length = img_name.split('-')[-2]\n",
    "    if len(check_length) == 7:\n",
    "        img_name = img_name[:18] + '0' + img_name[18:]\n",
    "    check_length = img_name.split('-')[-1]\n",
    "    if len(check_length) == 10:\n",
    "        img_name = img_name[:-5] + '0' + img_name[-5:]\n",
    "    img_name = img_name[:-6] + '_' + img_name[-6:]\n",
    "   \n",
    "    #path of each class\n",
    "    path0 = path+'\\\\class0'\n",
    "    path1 = path+'\\\\class1'\n",
    "    path2 = path+'\\\\class2'\n",
    "    path3 = path+'\\\\class3'\n",
    "    path4 = path+'\\\\class4'\n",
    "   \n",
    "    #creates folders to store images from eacg category, if not exists already\n",
    "    #requires dirs to exist - otherwise enters infinite loop and we have to restrat script\n",
    "    if not os.path.isdir(path0):\n",
    "        path = path4\n",
    "        os.mkdir(path)\n",
    "        path = path3\n",
    "        os.mkdir(path)\n",
    "        path = path2\n",
    "        os.mkdir(path)\n",
    "        path = path1\n",
    "        os.mkdir(path)\n",
    "        path = path0\n",
    "        os.mkdir(path)\n",
    "\n",
    "    #creates copy of image based on label\n",
    "    maj_vote = dataframe[dataframe['Sperm_Pic'].str.contains(img_name)]['Majority_Vote']\n",
    "\n",
    "    if maj_vote.values == 4:\n",
    "        shutil.copy(image, path4)\n",
    "    if maj_vote.values == 3:\n",
    "        shutil.copy(image, path3)\n",
    "    if maj_vote.values == 2:\n",
    "        shutil.copy(image, path2)\n",
    "    if maj_vote.values == 1:\n",
    "        shutil.copy(image, path1)\n",
    "    if maj_vote.values == 0:\n",
    "        shutil.copy(image, path0)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing to images on each folder\n",
    "dataframe['Majority_Vote'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Create Folders for Training, Validation and Test Data\n",
    "The Current solution is somewhat innefficient because it requires the creation of 2 copies of image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_dir = ['/class0', '/class1', '/class2', '/class3', '/class4']\n",
    "\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "if not os.path.isdir(path + '/train'):\n",
    "\n",
    "    for cls in classes_dir:\n",
    "    \n",
    "        #creates train and test folders, with each class separated inside\n",
    "        os.makedirs(path +'/train' + cls)\n",
    "        os.makedirs(path +'/val' + cls)\n",
    "        os.makedirs(path +'/test' + cls)\n",
    "\n",
    "\n",
    "        # Creating partitions of the data after shuffeling\n",
    "        src = path + cls # Folder to copy images from\n",
    "\n",
    "        allFileNames = os.listdir(src)\n",
    "        np.random.shuffle(allFileNames)\n",
    "        \n",
    "        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),\n",
    "                                                              [int(len(allFileNames)* (1 - val_ratio - test_ratio)), \n",
    "                                                               int(len(allFileNames)* (1 - val_ratio))])\n",
    "        \n",
    "        train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]\n",
    "        val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]\n",
    "        test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]\n",
    "\n",
    "    \n",
    "        # Copy-pasting images\n",
    "        for name in train_FileNames:\n",
    "            shutil.copy(name, path +'/train' + cls)\n",
    "\n",
    "        for name in val_FileNames:\n",
    "            shutil.copy(name, path +'/val' + cls)\n",
    "\n",
    "        for name in test_FileNames:\n",
    "            shutil.copy(name, path +'/test' + cls)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Augment images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentation pipelines\n",
    "class_0 = Augmentor.Pipeline(path+'/train/class0')\n",
    "class_1 = Augmentor.Pipeline(path+'/train/class1')\n",
    "class_2 = Augmentor.Pipeline(path+'/train/class2')\n",
    "class_3 = Augmentor.Pipeline(path+'/train/class3')\n",
    "\n",
    "# Define different augmentations depending on the pipeline; options are limited since we're working with microscopy data\n",
    "\n",
    "#visit: https://augmentor.readthedocs.io/en/master/userguide/mainfeatures.html#rotating\n",
    "#options are:\n",
    "#Perspective skewing: does not make sense; microscopy data\n",
    "###Elastic distortion: could work, but I'm afraid it'll actually deform the shape of the sper cell, which is what is picked up to classify it; could be detremental...\n",
    "###Rotation: makes sense! But no more than 5 degrees left or right...\n",
    "#Shear: does not make sense?\n",
    "#Cropping: does not make sense.\n",
    "###Mirroring: yes! both vertically and horizontally, randomly.\n",
    "\n",
    "\n",
    "#rotate by a maximum of 5 degrees\n",
    "class_0.rotate(probability=0.7, max_left_rotation=5, max_right_rotation=5)\n",
    "class_1.rotate(probability=0.7, max_left_rotation=5, max_right_rotation=5)\n",
    "class_2.rotate(probability=0.7, max_left_rotation=5, max_right_rotation=5)\n",
    "class_3.rotate(probability=0.7, max_left_rotation=5, max_right_rotation=5)\n",
    "\n",
    "#mirroring, vertical or horizontal, randomly\n",
    "class_0.flip_random(probability=0.7)\n",
    "class_1.flip_random(probability=0.7)\n",
    "class_2.flip_random(probability=0.7)\n",
    "class_3.flip_random(probability=0.7)\n",
    "\n",
    "# Augment images to the same proportion as existing ones in class 4 (majority class)\n",
    "class_0.sample(393-60)\n",
    "class_1.sample(394-136)\n",
    "class_2.sample(394-45)\n",
    "class_3.sample(394-43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#to play around with these\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   rotation_range = 5, #rotates images from -5 to 5 degrees\n",
    "                                   width_shift_range = 0.06, #translates images by 6% to left or right\n",
    "                                   height_shift_range = 0.06, #translates images by 6% up and down\n",
    "                                   vertical_flip = True,\n",
    "                                   horizontal_flip = True,\n",
    "                                   brightness_range=[0.2,1.2], \n",
    "                                   fill_mode='nearest',\n",
    "                                   zoom_range = 0.2,\n",
    "                                   ) \n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "#test different color maps -  class modes and cross validation types\n",
    "training_set = train_datagen.flow_from_directory(path+'/train',\n",
    "                                                 target_size = (32, 32),\n",
    "                                                 batch_size = 32,\n",
    "                                                 shuffle = True,\n",
    "                                                 class_mode=\"categorical\",\n",
    "                                                 color_mode = 'rgb')\n",
    "\n",
    "val_set = val_datagen.flow_from_directory(path+'/val',\n",
    "                                            target_size = (32, 32),\n",
    "                                            batch_size = 32,\n",
    "                                            shuffle = True,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            color_mode = 'rgb')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(path+'/test',\n",
    "                                            target_size = (32, 32),\n",
    "                                            batch_size = 1,\n",
    "                                            shuffle = True,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            color_mode = 'rgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Very Simple Test Model\n",
    "\n",
    "To Test CROSS_VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_test_model():\n",
    "    '''creates an image classification model that uses 2 Convolutional CNNs layers (with maxpooling) and feeds the data through\n",
    "    a dense connected layer. This is a simple model to compute fast to test cross_validation'''\n",
    "    \n",
    "    cnn_model = keras.Sequential([\n",
    "\n",
    "        #convolutional layer with 32 3x3 filters - again, arbitrary,\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding = 'same', activation='relu'), \n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #2nd convolution with 128 filters\n",
    "        keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding = 'same', activation= 'relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "       \n",
    "        #MaxPooling - takes the max value of each 2x2 pool in the feature map\n",
    "        keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #the result of kthe CNN is then flattened and placed into the \n",
    "        keras.layers.Flatten(),\n",
    "        \n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #final layer, is output, 1 out of 5 possible results\n",
    "        #0 Normal, 1 Tapered, 2 Pyriform, 3 Small, 5 Amorphous\n",
    "        keras.layers.Dense(5, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define call-back early stopping criteria\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model: First try\n",
    "test_model = cnn_test_model()\n",
    "\n",
    "test_model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "history = test_model.fit(training_set,\n",
    "        epochs=500,\n",
    "        validation_data=val_set,\n",
    "        verbose = 1, \n",
    "        validation_steps = len(val_set),\n",
    "        callbacks = [es])\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = test_model.evaluate(training_set, verbose=1)\n",
    "_, val_acc = test_model.evaluate(val_set, verbose=1)\n",
    "print('Train: %.3f, Val: %.3f' % (train_acc, val_acc))\n",
    "\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#saves model\n",
    "test_model.save(r\"C:\\Users\\Ricardo Santos\\Desktop\\Mestrado Ricardo\\Ano 1\\Spring Semester\\Deep\\Deep_Learning_Project\\Models\\test_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance do primeiro teste\n",
    "\n",
    "test_loss, test_acc = test_model.evaluate(test_set)\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Summary\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted probality\n",
    "prediction = test_model.predict(test_set,verbose=1)\n",
    "\n",
    "#Get class of prediction\n",
    "predicted_class = np.argmax(prediction,axis=1)\n",
    "\n",
    "# get names of pictures\n",
    "filenames = test_set.filenames\n",
    "\n",
    "#get trueclass\n",
    "true_classes = test_set.classes\n",
    "\n",
    "#store info in dataframe\n",
    "df_predictions = pd.DataFrame({'Filename': filenames, 'Label': true_classes, 'Test': predicted_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and classification report\n",
    "class_labels = list(test_set.class_indices.keys())   \n",
    "print(confusion_matrix(true_classes, predicted_class))\n",
    "\n",
    "report = classification_report(true_classes, predicted_class, target_names=class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Design a More Complex and Robust Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_cnn_standard_model():\n",
    "    '''creates an image classification model that uses 2 Convolutional CNNs layers (with maxpooling) and feeds the data through\n",
    "    a dense connected layer. This is trial and error there is no specific reason for 2 layers'''\n",
    "    cnn_model = keras.Sequential([\n",
    "\n",
    "        #convolutional layer with 32 3x3 filters - again, arbitrary,\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding = 'same', activation='relu'), \n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #MaxPooling - takes the max value of each 2x2 pool in the feature map\n",
    "        keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #3rd convolution with 64 filters\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3,3),  padding = 'same', activation= 'relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #4rd convolution with 128 filters\n",
    "        keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding = 'same', activation= 'relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #Second MaxPool2D - to check with other options\n",
    "        keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #5th convolutional layer with 24 3x3 filters - again, arbitrary,\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(4,4), padding = 'same', activation='relu'), \n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #MaxPooling - takes the max value of each 2x2 pool in the feature map\n",
    "        #keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "\n",
    "        #second convolution with 36 filters\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding = 'same', activation= 'relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding = 'same', activation= 'relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #Second MaxPool2D - to check with other options\n",
    "        keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #convolutional layer with 24 3x3 filters - again, arbitrary,\n",
    "        keras.layers.Conv2D(filters=128, kernel_size=(4,4), padding = 'same', activation='relu'), \n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "\n",
    "        #second convolution with 36 filters\n",
    "        keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding = 'same', activation= 'relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding = 'same', activation= 'relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #Second MaxPool2D - to check with other options\n",
    "        keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #the result of kthe CNN is then flattened and placed into the \n",
    "        keras.layers.Flatten(),\n",
    "        \n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        #Add Dropout\n",
    "        keras.layers.Dropout(0.4),\n",
    "        \n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        #final layer, is output, 1 out of 5 possible results\n",
    "        #0 Normal, 1 Tapered, 2 Pyriform, 3 Small, 5 Amorphous\n",
    "        keras.layers.Dense(5, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model: First try\n",
    "cnn_model = image_cnn_standard_model()\n",
    "\n",
    "cnn_model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fit model\n",
    "history = cnn_model.fit(training_set,\n",
    "        epochs=500,\n",
    "        validation_data=val_set,\n",
    "        verbose = 1, \n",
    "        validation_steps = len(val_set),\n",
    "        callbacks = [es])\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = cnn_model.evaluate(training_set, verbose=1)\n",
    "_, val_acc = cnn_model.evaluate(val_set, verbose=1)\n",
    "print('Train: %.3f, Val: %.3f' % (train_acc, val_acc))\n",
    "\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "cnn_model.save(r\"C:\\Users\\Ricardo Santos\\Desktop\\Mestrado Ricardo\\Ano 1\\Spring Semester\\Deep\\Deep_Learning_Project\\Models\\model_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Summary\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test performance\n",
    "\n",
    "test_loss, test_acc = cnn_model.evaluate(test_set)\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions\n",
    "\n",
    "Code still needs improvement\n",
    "\n",
    "These predictions are gated by batch size\n",
    "\n",
    "Let's try and improve it with bruteforce Hyperparam optimization whiole being on the lookout for other ways to improve performance. Model is predicting all as Amorphous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted probality\n",
    "prediction = cnn_model.predict(test_set,verbose=1)\n",
    "\n",
    "#Get class of prediction\n",
    "predicted_class = np.argmax(prediction,axis=1)\n",
    "\n",
    "#get trueclass\n",
    "true_classes = test_set.classes\n",
    "\n",
    "# get names of pictures\n",
    "filenames = test_set.filenames\n",
    "\n",
    "#store info in dataframe\n",
    "robust_predictions = pd.DataFrame({'Filename': filenames,'Model_1': predicted_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join prediction to dataframe\n",
    "df_predictions = df_predictions.merge(robust_predictions, on = 'Filename', how = 'inner')\n",
    "\n",
    "del robust_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and classification report\n",
    "class_labels = list(test_set.class_indices.keys())   \n",
    "print(confusion_matrix(true_classes, predicted_class))\n",
    "\n",
    "report = classification_report(true_classes, predicted_class, target_names=class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 - Benchmarks and other tests\n",
    "\n",
    "Try and compare the models performance with other standardly used models:\n",
    "\n",
    "AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALEX NET\n",
    "\n",
    "#Importing library \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "#Instantiation\n",
    "AlexNet = Sequential()\n",
    "\n",
    "#1st Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=96, input_shape=(32,32,3), kernel_size=(11,11), strides=(4,4), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#2nd Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#3rd Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "#4th Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "#5th Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#Passing it to a Fully Connected layer\n",
    "AlexNet.add(Flatten())\n",
    "# 1st Fully Connected Layer\n",
    "AlexNet.add(Dense(4096, input_shape=(32,32,3,)))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "#2nd Fully Connected Layer\n",
    "AlexNet.add(Dense(4096))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "#Add Dropout\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "#3rd Fully Connected Layer\n",
    "AlexNet.add(Dense(1000))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "#Add Dropout\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "#Output Layer\n",
    "AlexNet.add(Dense(5))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('softmax'))\n",
    "\n",
    "#Model Summary\n",
    "AlexNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "AlexNet.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "\n",
    "history = AlexNet.fit(training_set,\n",
    "        epochs=500,\n",
    "        validation_data=val_set,\n",
    "        verbose = 1, \n",
    "        validation_steps = len(val_set),\n",
    "        callbacks = [es])\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = AlexNet.evaluate(training_set, verbose=1)\n",
    "_, val_acc = AlexNet.evaluate(val_set, verbose=1)\n",
    "print('Train: %.3f, Val: %.3f' % (train_acc, val_acc))\n",
    "\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "AlexNet.save(r\"C:\\Users\\Ricardo Santos\\Desktop\\Mestrado Ricardo\\Ano 1\\Spring Semester\\Deep\\Deep_Learning_Project\\Models\\Alex.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = AlexNet.evaluate(test_set)\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted probality\n",
    "prediction = AlexNet.predict(test_set,verbose=1)\n",
    "\n",
    "#Get class of prediction\n",
    "predicted_class = np.argmax(prediction,axis=1)\n",
    "\n",
    "#get trueclass\n",
    "true_classes = test_set.classes\n",
    "\n",
    "# get names of pictures\n",
    "filenames = test_set.filenames\n",
    "\n",
    "#store info in dataframe\n",
    "alex_predictions = pd.DataFrame({'Filename': filenames,'AlexNet': predicted_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join prediction to dataframe\n",
    "df_predictions = df_predictions.merge(alex_predictions, on = 'Filename', how = 'inner')\n",
    "\n",
    "del alex_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and classification report\n",
    "class_labels = list(test_set.class_indices.keys())   \n",
    "print(confusion_matrix(true_classes, predicted_class))\n",
    "\n",
    "report = classification_report(true_classes, predicted_class, target_names=class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning with RESNET50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet50 requires its own preprocessing \n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "#to play around with these\n",
    "res50train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   preprocessing_function=preprocess_input,\n",
    "                                   rotation_range = 5,\n",
    "                                   width_shift_range = 0.06, \n",
    "                                   height_shift_range = 0.06, \n",
    "                                   vertical_flip = True,\n",
    "                                   horizontal_flip = True,\n",
    "                                   shuffle = True,\n",
    "                                   brightness_range=[0.2,1.2], \n",
    "                                   fill_mode='nearest',\n",
    "                                   zoom_range = 0.2,\n",
    "                                   ) \n",
    "\n",
    "res50val_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function=preprocess_input)\n",
    "res50test_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function=preprocess_input)\n",
    "\n",
    "#test different color maps -  class modes and cross validation types\n",
    "res_train = res50train_datagen.flow_from_directory(path+'/train',\n",
    "                                                 target_size = (32, 32),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode=\"categorical\",\n",
    "                                                 color_mode = 'rgb',\n",
    "                                                 shuffle = True)\n",
    "\n",
    "res_val = res50val_datagen.flow_from_directory(path+'/val',\n",
    "                                            target_size = (32, 32),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            color_mode = 'rgb',\n",
    "                                            shuffle = True)\n",
    "\n",
    "res_test = res50test_datagen.flow_from_directory(path+'/test',\n",
    "                                            target_size = (32, 32),\n",
    "                                            batch_size = 1,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            color_mode = 'rgb',\n",
    "                                            shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "\n",
    "res50model = Sequential()\n",
    "\n",
    "#Layer 1: RES50 without top layer\n",
    "res50model.add(ResNet50(weights='imagenet', input_shape= (32,32,3),\n",
    "                 include_top = False, classes=5,))\n",
    "\n",
    "#Layer 2: RES50 without top layer\n",
    "res50model.add(Flatten())\n",
    "res50model.add(Dense(5, activation = 'softmax'))\n",
    "\n",
    "#freeze layers in resnet - weights obtained with IMAGENET challenge, we only train final layer\n",
    "res50model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile\n",
    "\n",
    "res50model.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "\n",
    "res_history = res50model.fit(res_train,\n",
    "        epochs=500,\n",
    "        validation_data=res_val,\n",
    "        verbose = 1, \n",
    "        validation_steps = len(val_set),\n",
    "        callbacks = [es])\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = res50model.evaluate(training_set, verbose=1)\n",
    "_, val_acc = res50model.evaluate(val_set, verbose=1)\n",
    "print('Train: %.3f, Val: %.3f' % (train_acc, val_acc))\n",
    "\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calling `save('res50_net.h5')`.\n",
    "res50model.save(r\"C:\\Users\\Ricardo Santos\\Desktop\\Mestrado Ricardo\\Ano 1\\Spring Semester\\Deep\\Deep_Learning_Project\\Models\\Res50_net.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = res50model.evaluate(res_test)\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted probality\n",
    "prediction = res50model.predict(res_test,verbose=1)\n",
    "\n",
    "#Get class of prediction\n",
    "predicted_class = np.argmax(prediction,axis=1)\n",
    "\n",
    "#get trueclass\n",
    "true_classes = res_test.classes\n",
    "\n",
    "# get names of pictures\n",
    "filenames = res_test.filenames\n",
    "\n",
    "#store info in dataframe\n",
    "res50_prebuilt_predictions = pd.DataFrame({'Filename': filenames,'ResNet50': predicted_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join prediction to dataframe\n",
    "df_predictions = df_predictions.merge(res50_prebuilt_predictions, on = 'Filename', how = 'inner')\n",
    "\n",
    "del res50_prebuilt_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and classification report\n",
    "class_labels = list(test_set.class_indices.keys())   \n",
    "print(confusion_matrix(true_classes, predicted_class))\n",
    "\n",
    "report = classification_report(true_classes, predicted_class, target_names=class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESNET50 Trained from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50model_2 = Sequential()\n",
    "\n",
    "#Layer 1: RES50 without top layer\n",
    "res50model_2.add(ResNet50(weights=None, input_shape= (32,32,3),\n",
    "                 include_top = False, classes=5,))\n",
    "\n",
    "#Layer 2: RES50 without top layer\n",
    "res50model_2.add(Flatten())\n",
    "res50model_2.add(Dense(5, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile\n",
    "\n",
    "res50model_2.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "\n",
    "res_history_2 = res50model_2.fit(res_train,\n",
    "        epochs=500,\n",
    "        validation_data=res_val,\n",
    "        verbose = 1, \n",
    "        validation_steps = len(res_val),\n",
    "        callbacks = [es])\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = res50model_2.evaluate(res_train, verbose=1)\n",
    "_, val_acc = res50model_2.evaluate(res_val, verbose=1)\n",
    "print('Train: %.3f, Val: %.3f' % (train_acc, val_acc))\n",
    "\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calling `save('my_model.h5')` creates a h5 file `my_model.h5`.\n",
    "res50model.save(r\"C:\\Users\\Ricardo Santos\\Desktop\\Mestrado Ricardo\\Ano 1\\Spring Semester\\Deep\\Deep_Learning_Project\\Models\\Res50_from_scratch.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance do primeiro teste\n",
    "\n",
    "test_loss, test_acc = res50model_2.evaluate(res_test)\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted probality\n",
    "prediction = res50model_2.predict(res_test,verbose=1)\n",
    "\n",
    "#Get class of prediction\n",
    "predicted_class = np.argmax(prediction,axis=1)\n",
    "\n",
    "#get trueclass\n",
    "true_classes = res_test.classes\n",
    "\n",
    "# get names of pictures\n",
    "filenames = res_test.filenames\n",
    "\n",
    "#store info in dataframe\n",
    "res50_predictions = pd.DataFrame({'Filename': filenames,'ResNet50 From Scratch': predicted_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join prediction to dataframe\n",
    "df_predictions = df_predictions.merge(res50_predictions, on = 'Filename', how = 'inner')\n",
    "\n",
    "del res50_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and classification report\n",
    "class_labels = list(test_set.class_indices.keys())   \n",
    "print(confusion_matrix(true_classes, predicted_class))\n",
    "\n",
    "report = classification_report(true_classes, predicted_class, target_names=class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR TOMORROW\n",
    "\n",
    "Check different loss functions\n",
    "CROSS VALIDATION\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
