{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project\n",
    "\n",
    "The current project relies on using CNNs in order to process sperm data. The goal is to develop a classifier able to identify sperm cells.\n",
    "\n",
    "The model is not yet fuly defined. We will start with a standard CNN to Dense Layer.\n",
    "\n",
    "The goal is for images to be loaded into the CNN. The CNN will will then perform feature extraction and those will be fed to the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Create the pipeline\n",
    "\n",
    "The Deep learning model will be made out of 2 different parts: \n",
    "\n",
    "1. A CNN that takes the images as inputs and performs feature extraction.\n",
    "2. A dense, fully connected layer that will perform classification itself with the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import some libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "#image manipulation packages\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Data Augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#Classification\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix\n",
    "\n",
    "sns.set()\n",
    "#import keras\n",
    "from tensorflow import keras\n",
    "\n",
    "import glob\n",
    "import cv2\n",
    "import shutil\n",
    "import random\n",
    "import Augmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6046140245079271735\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#Check Computer's available devices\n",
    "#Will need to check in the future\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Label images\n",
    "\n",
    "In this case, we have a folder with 1132 image - SCIAN-MorphoSpermGS folder - https://cimt.uchile.cl/gold10/. Each image is 35 x 35 pixels and has been classified by 3 experts. We will use majority vote result as target. Each image will need to be loaded and the dataset will need to be created.\n",
    "\n",
    "#### This will yield a dataset with the picture name and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the path where you've put your dataset provided in Moodle\n",
    "\n",
    "#step 1: change directory back\n",
    "os.chdir('../Dataset')\n",
    "path = os.getcwd()\n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p1-pl2-sample01/Sperm_01</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p1-pl2-sample01/Sperm_02</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p1-pl2-sample01/Sperm_03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p1-pl2-sample01/Sperm_04</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p1-pl2-sample01/Sperm_05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>p5-pl1-sample20/Sperm_07</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>p5-pl1-sample20/Sperm_09</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>p5-pl1-sample20/Sperm_11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>p5-pl1-sample20/Sperm_12</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>p5-pl1-sample20/Sperm_13</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1132 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0  1  2  3  4\n",
       "0     p1-pl2-sample01/Sperm_01  5  5  5  5\n",
       "1     p1-pl2-sample01/Sperm_02  1  5  5  5\n",
       "2     p1-pl2-sample01/Sperm_03  0  0  0  0\n",
       "3     p1-pl2-sample01/Sperm_04  1  5  5  5\n",
       "4     p1-pl2-sample01/Sperm_05  0  0  0  0\n",
       "...                        ... .. .. .. ..\n",
       "1127  p5-pl1-sample20/Sperm_07  1  5  5  5\n",
       "1128  p5-pl1-sample20/Sperm_09  1  1  5  1\n",
       "1129  p5-pl1-sample20/Sperm_11  1  1  5  1\n",
       "1130  p5-pl1-sample20/Sperm_12  5  5  5  5\n",
       "1131  p5-pl1-sample20/Sperm_13  1  5  5  5\n",
       "\n",
       "[1132 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 - load txt file\n",
    "dataframe = pd.read_csv(path + '\\PA-expert-annotations.txt', sep = '\\\\t', header = None, engine = 'python')\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Start by most standard preprocessing of Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sperm_Pic</th>\n",
       "      <th>Majority_Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ch00_p1-pl2-sample01-sperm_01.tif</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ch00_p1-pl2-sample01-sperm_02.tif</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ch00_p1-pl2-sample01-sperm_03.tif</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ch00_p1-pl2-sample01-sperm_04.tif</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ch00_p1-pl2-sample01-sperm_05.tif</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>ch00_p5-pl1-sample20-sperm_07.tif</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>ch00_p5-pl1-sample20-sperm_09.tif</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>ch00_p5-pl1-sample20-sperm_11.tif</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>ch00_p5-pl1-sample20-sperm_12.tif</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>ch00_p5-pl1-sample20-sperm_13.tif</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1132 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Sperm_Pic  Majority_Vote\n",
       "0     ch00_p1-pl2-sample01-sperm_01.tif              4\n",
       "1     ch00_p1-pl2-sample01-sperm_02.tif              4\n",
       "2     ch00_p1-pl2-sample01-sperm_03.tif              0\n",
       "3     ch00_p1-pl2-sample01-sperm_04.tif              4\n",
       "4     ch00_p1-pl2-sample01-sperm_05.tif              0\n",
       "...                                 ...            ...\n",
       "1127  ch00_p5-pl1-sample20-sperm_07.tif              4\n",
       "1128  ch00_p5-pl1-sample20-sperm_09.tif              1\n",
       "1129  ch00_p5-pl1-sample20-sperm_11.tif              1\n",
       "1130  ch00_p5-pl1-sample20-sperm_12.tif              4\n",
       "1131  ch00_p5-pl1-sample20-sperm_13.tif              4\n",
       "\n",
       "[1132 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#editions to target dataset\n",
    "dataframe.rename(columns={0 :\"Sperm_Pic\", 1: 'Expert_1', 2: 'Expert_2', 3: 'Expert_3', 4: 'Majority_Vote'}, inplace = True)\n",
    "dataframe['Sperm_Pic'] = dataframe['Sperm_Pic'].str.replace('/S', '-s')\n",
    "dataframe['Majority_Vote'] = dataframe['Majority_Vote'].replace(5, 4)\n",
    "\n",
    "\n",
    "#add ch_00 to all rows:\n",
    "dataframe['Sperm_Pic'] = 'ch00_' + dataframe['Sperm_Pic'] + '.tif'\n",
    "\n",
    "#Drop all irrelevant features\n",
    "dataframe = dataframe.drop(['Expert_1', 'Expert_2', 'Expert_3'], axis = 1)\n",
    "\n",
    "#Show\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Place images on Folder based on image Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Partial-Agreement-Images')\n",
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imgs = []\n",
    "\n",
    "for image in glob.glob(path+'/*.tif'):\n",
    "        \n",
    "    imgs.append(cv2.imread(image))\n",
    "    \n",
    "    #names of each sample in the image and dataframe are different, gotta deal with this here to match them\n",
    "\n",
    "    img_name = image.split('\\\\')[-1]\n",
    "    check_length = img_name.split('-')[-2]\n",
    "    if len(check_length) == 7:\n",
    "        img_name = img_name[:18] + '0' + img_name[18:]\n",
    "    check_length = img_name.split('-')[-1]\n",
    "    if len(check_length) == 10:\n",
    "        img_name = img_name[:-5] + '0' + img_name[-5:]\n",
    "    img_name = img_name[:-6] + '_' + img_name[-6:]\n",
    "   \n",
    "    #path of each class\n",
    "    path0 = path+'\\\\class0'\n",
    "    path1 = path+'\\\\class1'\n",
    "    path2 = path+'\\\\class2'\n",
    "    path3 = path+'\\\\class3'\n",
    "    path4 = path+'\\\\class4'\n",
    "   \n",
    "    #creates folders to store images from eacg category, if not exists already\n",
    "    #requires dirs to exist - otherwise enters infinite loop and we have to restrat script\n",
    "    if not os.path.isdir(path0):\n",
    "        path = path4\n",
    "        os.mkdir(path)\n",
    "        path = path3\n",
    "        os.mkdir(path)\n",
    "        path = path2\n",
    "        os.mkdir(path)\n",
    "        path = path1\n",
    "        os.mkdir(path)\n",
    "        path = path0\n",
    "        os.mkdir(path)\n",
    "\n",
    "    #creates copy of image based on label\n",
    "    maj_vote = dataframe[dataframe['Sperm_Pic'].str.contains(img_name)]['Majority_Vote']\n",
    "\n",
    "    if maj_vote.values == 4:\n",
    "        shutil.copy(image, path4)\n",
    "    if maj_vote.values == 3:\n",
    "        shutil.copy(image, path3)\n",
    "    if maj_vote.values == 2:\n",
    "        shutil.copy(image, path2)\n",
    "    if maj_vote.values == 1:\n",
    "        shutil.copy(image, path1)\n",
    "    if maj_vote.values == 0:\n",
    "        shutil.copy(image, path0)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    656\n",
       "1    228\n",
       "0    100\n",
       "2     76\n",
       "3     72\n",
       "Name: Majority_Vote, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#comparing to images on each folder\n",
    "dataframe['Majority_Vote'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Create Folders for Training, Validation and Test Data\n",
    "The Current solution is somewhat innefficient because it requires the creation of 2 copies of image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_dir = ['/class0', '/class1', '/class2', '/class3', '/class4']\n",
    "\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "if not os.path.isdir(path + '/train'):\n",
    "\n",
    "    for cls in classes_dir:\n",
    "    \n",
    "        #creates train and test folders, with each class separated inside\n",
    "        os.makedirs(path +'/train' + cls)\n",
    "        os.makedirs(path +'/val' + cls)\n",
    "        os.makedirs(path +'/test' + cls)\n",
    "\n",
    "\n",
    "        # Creating partitions of the data after shuffeling\n",
    "        src = path + cls # Folder to copy images from\n",
    "\n",
    "        allFileNames = os.listdir(src)\n",
    "        np.random.shuffle(allFileNames)\n",
    "        \n",
    "        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),\n",
    "                                                              [int(len(allFileNames)* (1 - val_ratio - test_ratio)), \n",
    "                                                               int(len(allFileNames)* (1 - val_ratio))])\n",
    "        \n",
    "        train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]\n",
    "        val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]\n",
    "        test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]\n",
    "\n",
    "    \n",
    "        # Copy-pasting images\n",
    "        for name in train_FileNames:\n",
    "            shutil.copy(name, path +'/train' + cls)\n",
    "\n",
    "        for name in val_FileNames:\n",
    "            shutil.copy(name, path +'/val' + cls)\n",
    "\n",
    "        for name in test_FileNames:\n",
    "            shutil.copy(name, path +'/test' + cls)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Augment images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=L size=35x35 at 0x1123D6C5400>:   0%|             | 0/333 [00:00<?, ? Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 60 image(s) found.\n",
      "Output directory set to C:\\Users\\Ricardo Santos\\Desktop\\Mestrado Ricardo\\Ano 1\\Spring Semester\\Deep\\Deep_Learning_Project\\Dataset\\Partial-Agreement-Images/train/class0\\output.Initialised with 136 image(s) found.\n",
      "Output directory set to C:\\Users\\Ricardo Santos\\Desktop\\Mestrado Ricardo\\Ano 1\\Spring Semester\\Deep\\Deep_Learning_Project\\Dataset\\Partial-Agreement-Images/train/class1\\output.Initialised with 45 image(s) found.\n",
      "Output directory set to C:\\Users\\Ricardo Santos\\Desktop\\Mestrado Ricardo\\Ano 1\\Spring Semester\\Deep\\Deep_Learning_Project\\Dataset\\Partial-Agreement-Images/train/class2\\output.Initialised with 43 image(s) found.\n",
      "Output directory set to C:\\Users\\Ricardo Santos\\Desktop\\Mestrado Ricardo\\Ano 1\\Spring Semester\\Deep\\Deep_Learning_Project\\Dataset\\Partial-Agreement-Images/train/class3\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=L size=35x35 at 0x1123D6CA700>: 100%|██| 333/333 [00:00<00:00, 562.50 Samples/s]\n",
      "Processing <PIL.TiffImagePlugin.TiffImageFile image mode=L size=35x35 at 0x1123DA63A00>: 100%|█| 258/258 [00:00<00:00, \n",
      "Processing <PIL.Image.Image image mode=L size=35x35 at 0x1123D6AD2E0>: 100%|██| 349/349 [00:00<00:00, 551.35 Samples/s]\n",
      "Processing <PIL.TiffImagePlugin.TiffImageFile image mode=L size=35x35 at 0x1123DA17940>: 100%|█| 351/351 [00:00<00:00, \n"
     ]
    }
   ],
   "source": [
    "# Define augmentation pipelines\n",
    "class_0 = Augmentor.Pipeline(path+'/train/class0')\n",
    "class_1 = Augmentor.Pipeline(path+'/train/class1')\n",
    "class_2 = Augmentor.Pipeline(path+'/train/class2')\n",
    "class_3 = Augmentor.Pipeline(path+'/train/class3')\n",
    "\n",
    "# Define different augmentations depending on the pipeline; options are limited since we're working with microscopy data\n",
    "\n",
    "#visit: https://augmentor.readthedocs.io/en/master/userguide/mainfeatures.html#rotating\n",
    "#options are:\n",
    "#Perspective skewing: does not make sense; microscopy data\n",
    "###Elastic distortion: could work, but I'm afraid it'll actually deform the shape of the sper cell, which is what is picked up to classify it; could be detremental...\n",
    "###Rotation: makes sense! But no more than 5 degrees left or right...\n",
    "#Shear: does not make sense?\n",
    "#Cropping: does not make sense.\n",
    "###Mirroring: yes! both vertically and horizontally, randomly.\n",
    "\n",
    "\n",
    "#rotate by a maximum of 5 degrees\n",
    "class_0.rotate(probability=0.7, max_left_rotation=5, max_right_rotation=5)\n",
    "class_1.rotate(probability=0.7, max_left_rotation=5, max_right_rotation=5)\n",
    "class_2.rotate(probability=0.7, max_left_rotation=5, max_right_rotation=5)\n",
    "class_3.rotate(probability=0.7, max_left_rotation=5, max_right_rotation=5)\n",
    "\n",
    "#mirroring, vertical or horizontal, randomly\n",
    "class_0.flip_random(probability=0.7)\n",
    "class_1.flip_random(probability=0.7)\n",
    "class_2.flip_random(probability=0.7)\n",
    "class_3.flip_random(probability=0.7)\n",
    "\n",
    "# Augment images to the same proportion as existing ones in class 4 (majority class)\n",
    "class_0.sample(393-60)\n",
    "class_1.sample(394-136)\n",
    "class_2.sample(394-45)\n",
    "class_3.sample(394-43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5841 images belonging to 5 classes.\n",
      "Found 226 images belonging to 5 classes.\n",
      "Found 229 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "#load image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#to play around with these\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   rotation_range = 5, #rotates images from -5 to 5 degrees\n",
    "                                   width_shift_range = 0.06, #translates images by 6% to left or right\n",
    "                                   height_shift_range = 0.06, #translates images by 6% up and down\n",
    "                                   vertical_flip = True,\n",
    "                                   horizontal_flip = True,\n",
    "                                   shuffle = True,\n",
    "                                   brightness_range=[0.2,1.2], \n",
    "                                   fill_mode='nearest',\n",
    "                                   zoom_range = 0.2,\n",
    "                                   ) \n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "#test different color maps -  class modes and cross validation types\n",
    "training_set = train_datagen.flow_from_directory(path+'/train',\n",
    "                                                 target_size = (32, 32),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode=\"categorical\",\n",
    "                                                 color_mode = 'rgb')\n",
    "\n",
    "val_set = val_datagen.flow_from_directory(path+'/val',\n",
    "                                            target_size = (32, 32),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            color_mode = 'rgb')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(path+'/test',\n",
    "                                            target_size = (32, 32),\n",
    "                                            batch_size = 1,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            color_mode = 'rgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Very Simple Test Model\n",
    "\n",
    "To Test CROSS_VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_test_model():\n",
    "    '''creates an image classification model that uses 2 Convolutional CNNs layers (with maxpooling) and feeds the data through\n",
    "    a dense connected layer. This is a simple model to compute fast to test cross_validation'''\n",
    "    \n",
    "    cnn_model = keras.Sequential([\n",
    "\n",
    "        #convolutional layer with 32 3x3 filters - again, arbitrary,\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding = 'same', activation='relu'), \n",
    "        keras.layers.normalization.BatchNormalization(),\n",
    "        \n",
    "        #2nd convolution with 128 filters\n",
    "        keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding = 'same', activation= 'relu'),\n",
    "        keras.layers.normalization.BatchNormalization(),\n",
    "       \n",
    "        #MaxPooling - takes the max value of each 2x2 pool in the feature map\n",
    "        keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "        keras.layers.normalization.BatchNormalization(),\n",
    "        \n",
    "        #the result of kthe CNN is then flattened and placed into the \n",
    "        keras.layers.Flatten(),\n",
    "        \n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.normalization.BatchNormalization(),\n",
    "        \n",
    "        #final layer, is output, 1 out of 5 possible results\n",
    "        #0 Normal, 1 Tapered, 2 Pyriform, 3 Small, 5 Amorphous\n",
    "        keras.layers.Dense(5, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define call-back early stopping criteria\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model: First try\n",
    "test_model = cnn_test_model()\n",
    "\n",
    "test_model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "history = test_model.fit(training_set,\n",
    "        epochs=500,\n",
    "        validation_data=val_set,\n",
    "        verbose = 1, \n",
    "        validation_steps = len(val_set),\n",
    "        callbacks = [es])\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = cnn_model.evaluate(training_set, verbose=1)\n",
    "_, val_acc = cnn_model.evaluate(val_set, verbose=1)\n",
    "print('Train: %.3f, Val: %.3f' % (train_acc, val_acc))\n",
    "\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#saves model\n",
    "test_model.save(\"test_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance do primeiro teste\n",
    "\n",
    "test_loss, test_acc = test_model.evaluate(test_set)\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Summary\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted probality\n",
    "prediction = test_model.predict_generator(test_set,verbose=1)\n",
    "\n",
    "#Get class of prediction\n",
    "predicted_class = np.argmax(prediction,axis=1)\n",
    "\n",
    "# get names of pictures\n",
    "filenames = test_set.filenames\n",
    "\n",
    "#store info in dataframe\n",
    "df_predictions = pd.DataFrame({'Filename': filenames,'Test': prednames})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Design a More Complex and Robust Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_cnn_standard_model():\n",
    "    '''creates an image classification model that uses 2 Convolutional CNNs layers (with maxpooling) and feeds the data through\n",
    "    a dense connected layer. This is trial and error there is no specific reason for 2 layers'''\n",
    "    cnn_model = keras.Sequential([\n",
    "\n",
    "        #convolutional layer with 32 3x3 filters - again, arbitrary,\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding = 'same', activation='relu'), \n",
    "        keras.layers.normalization.BatchNormalization(),\n",
    "        \n",
    "        #MaxPooling - takes the max value of each 2x2 pool in the feature map\n",
    "        keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "        keras.layers.normalization.BatchNormalization(),\n",
    "        \n",
    "        #3rd convolution with 64 filters\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3,3),  padding = 'same', activation= 'relu'),\n",
    "        keras.layers.normalization.BatchNormalization(),\n",
    "        \n",
    "        #4rd convolution with 128 filters\n",
    "        keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding = 'same', activation= 'relu'),\n",
    "        keras.layers.normalization.BatchNormalization(),\n",
    "        \n",
    "        #Second MaxPool2D - to check with other options\n",
    "        keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "        keras.layers.normalization.BatchNormalization(),\n",
    "        \n",
    "        #5th convolutional layer with 24 3x3 filters - again, arbitrary,\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(4,4), padding = 'same', activation='relu'), \n",
    "        keras.layers.normalization.BatchNormalization(),\n",
    "        \n",
    "        #MaxPooling - takes the max value of each 2x2 pool in the feature map\n",
    "        #keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "\n",
    "        #second convolution with 36 filters\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding = 'same', activation= 'relu'),\n",
    "        keras.layers.normalization.BatchNormalization(),\n",
    "        \n",
    "        keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding = 'same', activation= 'relu'),\n",
    "        keras.layers.normalization.BatchNormalization(),\n",
    "        \n",
    "        #Second MaxPool2D - to check with other options\n",
    "        keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "        keras.layers.normalization.BatchNormalization(),\n",
    "        \n",
    "        #convolutional layer with 24 3x3 filters - again, arbitrary,\n",
    "        keras.layers.Conv2D(filters=128, kernel_size=(4,4), padding = 'same', activation='relu'), \n",
    "        keras.layers.normalization.BatchNormalization(),\n",
    "        \n",
    "\n",
    "        #second convolution with 36 filters\n",
    "        keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding = 'same', activation= 'relu'),\n",
    "        keras.layers.normalization.BatchNormalization(),\n",
    "        \n",
    "        keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding = 'same', activation= 'relu'),\n",
    "        keras.layers.normalization.BatchNormalization(),\n",
    "        \n",
    "        #Second MaxPool2D - to check with other options\n",
    "        keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "        keras.layers.normalization.BatchNormalization(),\n",
    "        \n",
    "        #the result of kthe CNN is then flattened and placed into the \n",
    "        keras.layers.Flatten(),\n",
    "        \n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.normalization.BatchNormalization(),\n",
    "        #Add Dropout\n",
    "        keras.layers.Dropout(0.4),\n",
    "        \n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.normalization.BatchNormalization(),\n",
    "        #final layer, is output, 1 out of 5 possible results\n",
    "        #0 Normal, 1 Tapered, 2 Pyriform, 3 Small, 5 Amorphous\n",
    "        keras.layers.Dense(5, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model: First try\n",
    "cnn_model = image_cnn_standard_model()\n",
    "\n",
    "cnn_model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "183/183 [==============================] - 26s 134ms/step - loss: 1.7800 - accuracy: 0.2531 - val_loss: 2.1326 - val_accuracy: 0.0664\n",
      "Epoch 2/500\n",
      "183/183 [==============================] - 17s 91ms/step - loss: 1.4100 - accuracy: 0.4049 - val_loss: 3.0672 - val_accuracy: 0.0708\n",
      "Epoch 3/500\n",
      "183/183 [==============================] - 16s 90ms/step - loss: 1.2711 - accuracy: 0.4856 - val_loss: 1.7730 - val_accuracy: 0.1947\n",
      "Epoch 4/500\n",
      "183/183 [==============================] - 18s 98ms/step - loss: 1.1827 - accuracy: 0.5378 - val_loss: 2.1313 - val_accuracy: 0.2212\n",
      "Epoch 5/500\n",
      "183/183 [==============================] - 17s 95ms/step - loss: 1.1128 - accuracy: 0.5644 - val_loss: 2.0507 - val_accuracy: 0.2920\n",
      "Epoch 6/500\n",
      "183/183 [==============================] - 17s 95ms/step - loss: 1.0422 - accuracy: 0.6120 - val_loss: 1.8231 - val_accuracy: 0.2699\n",
      "Epoch 7/500\n",
      "183/183 [==============================] - 17s 95ms/step - loss: 1.0066 - accuracy: 0.6247 - val_loss: 2.0861 - val_accuracy: 0.2611\n",
      "Epoch 8/500\n",
      "183/183 [==============================] - 17s 93ms/step - loss: 0.9471 - accuracy: 0.6503 - val_loss: 1.9756 - val_accuracy: 0.2655\n",
      "Epoch 9/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.8835 - accuracy: 0.6796 - val_loss: 1.9024 - val_accuracy: 0.2876\n",
      "Epoch 10/500\n",
      "183/183 [==============================] - 17s 93ms/step - loss: 0.8744 - accuracy: 0.6880 - val_loss: 1.9889 - val_accuracy: 0.2611\n",
      "Epoch 11/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.8522 - accuracy: 0.7011 - val_loss: 2.0842 - val_accuracy: 0.2566\n",
      "Epoch 12/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.8212 - accuracy: 0.7088 - val_loss: 2.3092 - val_accuracy: 0.3053\n",
      "Epoch 13/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.7919 - accuracy: 0.7198 - val_loss: 1.9563 - val_accuracy: 0.3009\n",
      "Epoch 14/500\n",
      "183/183 [==============================] - 17s 95ms/step - loss: 0.7393 - accuracy: 0.7324 - val_loss: 1.9577 - val_accuracy: 0.3363\n",
      "Epoch 15/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.7406 - accuracy: 0.7375 - val_loss: 1.9289 - val_accuracy: 0.2965\n",
      "Epoch 16/500\n",
      "183/183 [==============================] - 17s 93ms/step - loss: 0.6789 - accuracy: 0.7633 - val_loss: 1.8905 - val_accuracy: 0.3097\n",
      "Epoch 17/500\n",
      "183/183 [==============================] - 17s 93ms/step - loss: 0.6718 - accuracy: 0.7669 - val_loss: 2.1806 - val_accuracy: 0.3053\n",
      "Epoch 18/500\n",
      "183/183 [==============================] - 18s 98ms/step - loss: 0.6441 - accuracy: 0.7721 - val_loss: 1.9357 - val_accuracy: 0.3186\n",
      "Epoch 19/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.5873 - accuracy: 0.7998 - val_loss: 1.8846 - val_accuracy: 0.3053\n",
      "Epoch 20/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.5567 - accuracy: 0.8109 - val_loss: 2.0068 - val_accuracy: 0.3009\n",
      "Epoch 21/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.5614 - accuracy: 0.8023 - val_loss: 1.7759 - val_accuracy: 0.3451\n",
      "Epoch 22/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.4943 - accuracy: 0.8342 - val_loss: 2.0614 - val_accuracy: 0.3142\n",
      "Epoch 23/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.4797 - accuracy: 0.8384 - val_loss: 2.2277 - val_accuracy: 0.2876\n",
      "Epoch 24/500\n",
      "183/183 [==============================] - 17s 93ms/step - loss: 0.4912 - accuracy: 0.8375 - val_loss: 1.7290 - val_accuracy: 0.3673\n",
      "Epoch 25/500\n",
      "183/183 [==============================] - 17s 93ms/step - loss: 0.4500 - accuracy: 0.8480 - val_loss: 1.5541 - val_accuracy: 0.4071\n",
      "Epoch 26/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.4252 - accuracy: 0.8496 - val_loss: 1.6393 - val_accuracy: 0.4027\n",
      "Epoch 27/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.4289 - accuracy: 0.8534 - val_loss: 1.4816 - val_accuracy: 0.4425\n",
      "Epoch 28/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.4157 - accuracy: 0.8532 - val_loss: 1.6300 - val_accuracy: 0.4336\n",
      "Epoch 29/500\n",
      "183/183 [==============================] - 17s 93ms/step - loss: 0.3854 - accuracy: 0.8697 - val_loss: 1.8434 - val_accuracy: 0.4248\n",
      "Epoch 30/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.4029 - accuracy: 0.8645 - val_loss: 1.5712 - val_accuracy: 0.4336\n",
      "Epoch 31/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.3599 - accuracy: 0.8820 - val_loss: 1.6275 - val_accuracy: 0.4469\n",
      "Epoch 32/500\n",
      "183/183 [==============================] - 17s 92ms/step - loss: 0.3547 - accuracy: 0.8809 - val_loss: 2.0627 - val_accuracy: 0.3628\n",
      "Epoch 33/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.3316 - accuracy: 0.8932 - val_loss: 1.9192 - val_accuracy: 0.4027\n",
      "Epoch 34/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.3523 - accuracy: 0.8773 - val_loss: 1.4845 - val_accuracy: 0.4735\n",
      "Epoch 35/500\n",
      "183/183 [==============================] - 17s 93ms/step - loss: 0.3585 - accuracy: 0.8768 - val_loss: 1.5151 - val_accuracy: 0.4602\n",
      "Epoch 36/500\n",
      "183/183 [==============================] - 17s 93ms/step - loss: 0.3134 - accuracy: 0.8985 - val_loss: 1.8227 - val_accuracy: 0.3717\n",
      "Epoch 37/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.3090 - accuracy: 0.8959 - val_loss: 1.5337 - val_accuracy: 0.4469\n",
      "Epoch 38/500\n",
      "183/183 [==============================] - 17s 95ms/step - loss: 0.3170 - accuracy: 0.8970 - val_loss: 1.3243 - val_accuracy: 0.5487\n",
      "Epoch 39/500\n",
      "183/183 [==============================] - 17s 93ms/step - loss: 0.2886 - accuracy: 0.9061 - val_loss: 1.7226 - val_accuracy: 0.4381\n",
      "Epoch 40/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.3136 - accuracy: 0.8912 - val_loss: 2.3126 - val_accuracy: 0.3673\n",
      "Epoch 41/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.2924 - accuracy: 0.9017 - val_loss: 2.0285 - val_accuracy: 0.4027\n",
      "Epoch 42/500\n",
      "183/183 [==============================] - 17s 93ms/step - loss: 0.2894 - accuracy: 0.9063 - val_loss: 1.7297 - val_accuracy: 0.4115\n",
      "Epoch 43/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.2898 - accuracy: 0.9011 - val_loss: 1.5038 - val_accuracy: 0.5044\n",
      "Epoch 44/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.2583 - accuracy: 0.9108 - val_loss: 1.7705 - val_accuracy: 0.4381\n",
      "Epoch 45/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.2762 - accuracy: 0.9121 - val_loss: 1.5123 - val_accuracy: 0.5354\n",
      "Epoch 46/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.2638 - accuracy: 0.9093 - val_loss: 1.8056 - val_accuracy: 0.4204\n",
      "Epoch 47/500\n",
      "183/183 [==============================] - 17s 95ms/step - loss: 0.2482 - accuracy: 0.9212 - val_loss: 1.8322 - val_accuracy: 0.4469\n",
      "Epoch 48/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.2360 - accuracy: 0.9233 - val_loss: 1.4496 - val_accuracy: 0.4823\n",
      "Epoch 49/500\n",
      "183/183 [==============================] - 17s 93ms/step - loss: 0.2595 - accuracy: 0.9172 - val_loss: 1.4018 - val_accuracy: 0.4956\n",
      "Epoch 50/500\n",
      "183/183 [==============================] - 17s 93ms/step - loss: 0.2464 - accuracy: 0.9156 - val_loss: 1.5865 - val_accuracy: 0.4735\n",
      "Epoch 51/500\n",
      "183/183 [==============================] - 21s 114ms/step - loss: 0.2175 - accuracy: 0.9309 - val_loss: 1.5112 - val_accuracy: 0.4735\n",
      "Epoch 52/500\n",
      "183/183 [==============================] - 19s 102ms/step - loss: 0.2106 - accuracy: 0.9290 - val_loss: 1.5584 - val_accuracy: 0.4779\n",
      "Epoch 53/500\n",
      "183/183 [==============================] - 18s 101ms/step - loss: 0.2228 - accuracy: 0.9260 - val_loss: 1.4247 - val_accuracy: 0.5133\n",
      "Epoch 54/500\n",
      "183/183 [==============================] - 18s 97ms/step - loss: 0.2154 - accuracy: 0.9271 - val_loss: 1.5081 - val_accuracy: 0.4867TA: 1s - loss: 0.2150 - accuracy - ETA: 1s - loss:\n",
      "Epoch 55/500\n",
      "183/183 [==============================] - 18s 97ms/step - loss: 0.1946 - accuracy: 0.9353 - val_loss: 1.3377 - val_accuracy: 0.5664\n",
      "Epoch 56/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 17s 94ms/step - loss: 0.2171 - accuracy: 0.9281 - val_loss: 1.3789 - val_accuracy: 0.5619\n",
      "Epoch 57/500\n",
      "183/183 [==============================] - 17s 93ms/step - loss: 0.2151 - accuracy: 0.9252 - val_loss: 1.6536 - val_accuracy: 0.5265\n",
      "Epoch 58/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.1852 - accuracy: 0.9409 - val_loss: 1.5052 - val_accuracy: 0.5531\n",
      "Epoch 59/500\n",
      "183/183 [==============================] - 17s 93ms/step - loss: 0.1843 - accuracy: 0.9393 - val_loss: 1.3932 - val_accuracy: 0.5442\n",
      "Epoch 60/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.1998 - accuracy: 0.9347 - val_loss: 1.2449 - val_accuracy: 0.5752\n",
      "Epoch 61/500\n",
      "183/183 [==============================] - 17s 95ms/step - loss: 0.1764 - accuracy: 0.9420 - val_loss: 1.5433 - val_accuracy: 0.5310\n",
      "Epoch 62/500\n",
      "183/183 [==============================] - 17s 95ms/step - loss: 0.1670 - accuracy: 0.9421 - val_loss: 1.5406 - val_accuracy: 0.5133\n",
      "Epoch 63/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.1804 - accuracy: 0.9400 - val_loss: 1.4523 - val_accuracy: 0.5354\n",
      "Epoch 64/500\n",
      "183/183 [==============================] - 17s 95ms/step - loss: 0.1832 - accuracy: 0.9407 - val_loss: 1.1893 - val_accuracy: 0.6195\n",
      "Epoch 65/500\n",
      "183/183 [==============================] - 18s 99ms/step - loss: 0.1741 - accuracy: 0.9448 - val_loss: 1.6807 - val_accuracy: 0.5221\n",
      "Epoch 66/500\n",
      "183/183 [==============================] - 19s 101ms/step - loss: 0.1868 - accuracy: 0.9346 - val_loss: 1.5768 - val_accuracy: 0.5442\n",
      "Epoch 67/500\n",
      "183/183 [==============================] - 18s 96ms/step - loss: 0.1644 - accuracy: 0.9471 - val_loss: 1.4851 - val_accuracy: 0.5265\n",
      "Epoch 68/500\n",
      "183/183 [==============================] - 17s 95ms/step - loss: 0.1627 - accuracy: 0.9475 - val_loss: 1.5328 - val_accuracy: 0.5531\n",
      "Epoch 69/500\n",
      "183/183 [==============================] - 18s 96ms/step - loss: 0.1530 - accuracy: 0.9500 - val_loss: 1.4946 - val_accuracy: 0.5442\n",
      "Epoch 70/500\n",
      "183/183 [==============================] - 17s 90ms/step - loss: 0.1690 - accuracy: 0.9474 - val_loss: 1.3713 - val_accuracy: 0.5752\n",
      "Epoch 71/500\n",
      "183/183 [==============================] - 17s 91ms/step - loss: 0.1539 - accuracy: 0.9510 - val_loss: 1.4652 - val_accuracy: 0.5531\n",
      "Epoch 72/500\n",
      "183/183 [==============================] - 18s 96ms/step - loss: 0.1532 - accuracy: 0.9495 - val_loss: 1.4569 - val_accuracy: 0.5752\n",
      "Epoch 73/500\n",
      "183/183 [==============================] - 17s 91ms/step - loss: 0.1654 - accuracy: 0.9475 - val_loss: 1.3532 - val_accuracy: 0.5841\n",
      "Epoch 74/500\n",
      "183/183 [==============================] - 17s 92ms/step - loss: 0.1504 - accuracy: 0.9525 - val_loss: 1.4618 - val_accuracy: 0.5664\n",
      "Epoch 75/500\n",
      "183/183 [==============================] - 17s 92ms/step - loss: 0.1539 - accuracy: 0.9495 - val_loss: 1.7692 - val_accuracy: 0.5221\n",
      "Epoch 76/500\n",
      "183/183 [==============================] - 16s 90ms/step - loss: 0.1468 - accuracy: 0.9507 - val_loss: 1.5474 - val_accuracy: 0.5133\n",
      "Epoch 77/500\n",
      "183/183 [==============================] - 16s 89ms/step - loss: 0.1572 - accuracy: 0.9488 - val_loss: 1.3109 - val_accuracy: 0.6018\n",
      "Epoch 78/500\n",
      "183/183 [==============================] - 17s 90ms/step - loss: 0.1292 - accuracy: 0.9568 - val_loss: 1.4147 - val_accuracy: 0.5752\n",
      "Epoch 79/500\n",
      "183/183 [==============================] - 17s 90ms/step - loss: 0.1563 - accuracy: 0.9532 - val_loss: 1.6646 - val_accuracy: 0.5487\n",
      "Epoch 80/500\n",
      "183/183 [==============================] - 17s 91ms/step - loss: 0.1491 - accuracy: 0.9485 - val_loss: 1.5695 - val_accuracy: 0.5575\n",
      "Epoch 81/500\n",
      "183/183 [==============================] - 18s 97ms/step - loss: 0.1423 - accuracy: 0.9525 - val_loss: 1.5484 - val_accuracy: 0.5398\n",
      "Epoch 82/500\n",
      "183/183 [==============================] - 19s 101ms/step - loss: 0.1134 - accuracy: 0.9613 - val_loss: 1.5017 - val_accuracy: 0.5619\n",
      "Epoch 83/500\n",
      "183/183 [==============================] - 22s 118ms/step - loss: 0.1131 - accuracy: 0.9604 - val_loss: 1.3710 - val_accuracy: 0.5841\n",
      "Epoch 84/500\n",
      "183/183 [==============================] - 20s 107ms/step - loss: 0.1223 - accuracy: 0.9608 - val_loss: 1.3483 - val_accuracy: 0.6504\n",
      "Epoch 85/500\n",
      "183/183 [==============================] - 20s 109ms/step - loss: 0.1389 - accuracy: 0.9571 - val_loss: 1.4316 - val_accuracy: 0.5841\n",
      "Epoch 86/500\n",
      "183/183 [==============================] - 22s 119ms/step - loss: 0.1340 - accuracy: 0.9571 - val_loss: 1.6196 - val_accuracy: 0.5177\n",
      "Epoch 87/500\n",
      "183/183 [==============================] - 21s 116ms/step - loss: 0.1551 - accuracy: 0.9508 - val_loss: 1.2597 - val_accuracy: 0.6372\n",
      "Epoch 88/500\n",
      "183/183 [==============================] - 20s 112ms/step - loss: 0.1250 - accuracy: 0.9560 - val_loss: 1.5336 - val_accuracy: 0.5708\n",
      "Epoch 89/500\n",
      "183/183 [==============================] - 21s 117ms/step - loss: 0.1612 - accuracy: 0.9471 - val_loss: 1.2417 - val_accuracy: 0.6504\n",
      "Epoch 90/500\n",
      "183/183 [==============================] - 20s 108ms/step - loss: 0.1299 - accuracy: 0.9575 - val_loss: 1.5703 - val_accuracy: 0.5531\n",
      "Epoch 91/500\n",
      "183/183 [==============================] - 20s 112ms/step - loss: 0.1163 - accuracy: 0.9587 - val_loss: 1.7052 - val_accuracy: 0.5531\n",
      "Epoch 92/500\n",
      "183/183 [==============================] - 20s 110ms/step - loss: 0.1337 - accuracy: 0.9550 - val_loss: 1.5341 - val_accuracy: 0.5929\n",
      "Epoch 93/500\n",
      "183/183 [==============================] - 21s 112ms/step - loss: 0.1150 - accuracy: 0.9641 - val_loss: 1.5423 - val_accuracy: 0.6018\n",
      "Epoch 94/500\n",
      "183/183 [==============================] - 21s 114ms/step - loss: 0.1247 - accuracy: 0.9587 - val_loss: 1.5294 - val_accuracy: 0.5752\n",
      "Epoch 95/500\n",
      "183/183 [==============================] - 21s 114ms/step - loss: 0.0987 - accuracy: 0.9684 - val_loss: 1.4380 - val_accuracy: 0.6106\n",
      "Epoch 96/500\n",
      "183/183 [==============================] - 21s 113ms/step - loss: 0.1098 - accuracy: 0.9656 - val_loss: 1.5569 - val_accuracy: 0.5708\n",
      "Epoch 97/500\n",
      "183/183 [==============================] - 21s 114ms/step - loss: 0.1131 - accuracy: 0.9604 - val_loss: 1.4303 - val_accuracy: 0.5752\n",
      "Epoch 98/500\n",
      "183/183 [==============================] - 20s 111ms/step - loss: 0.1068 - accuracy: 0.9634 - val_loss: 1.5820 - val_accuracy: 0.5796\n",
      "Epoch 99/500\n",
      "183/183 [==============================] - 20s 111ms/step - loss: 0.1161 - accuracy: 0.9603 - val_loss: 1.5794 - val_accuracy: 0.5708\n",
      "Epoch 100/500\n",
      "183/183 [==============================] - 22s 120ms/step - loss: 0.1034 - accuracy: 0.9616 - val_loss: 1.4990 - val_accuracy: 0.6416\n",
      "Epoch 101/500\n",
      "183/183 [==============================] - 21s 112ms/step - loss: 0.0950 - accuracy: 0.9680 - val_loss: 1.5240 - val_accuracy: 0.5796\n",
      "Epoch 102/500\n",
      "183/183 [==============================] - 21s 113ms/step - loss: 0.1102 - accuracy: 0.9606 - val_loss: 1.5438 - val_accuracy: 0.5531\n",
      "Epoch 103/500\n",
      "183/183 [==============================] - 21s 115ms/step - loss: 0.0855 - accuracy: 0.9686 - val_loss: 1.6572 - val_accuracy: 0.5619\n",
      "Epoch 104/500\n",
      "183/183 [==============================] - 20s 110ms/step - loss: 0.1128 - accuracy: 0.9637 - val_loss: 1.4972 - val_accuracy: 0.6062\n",
      "Epoch 105/500\n",
      "183/183 [==============================] - 20s 109ms/step - loss: 0.0936 - accuracy: 0.9682 - val_loss: 1.4996 - val_accuracy: 0.5796\n",
      "Epoch 106/500\n",
      "183/183 [==============================] - 20s 112ms/step - loss: 0.1017 - accuracy: 0.9677 - val_loss: 1.5939 - val_accuracy: 0.5619\n",
      "Epoch 107/500\n",
      "183/183 [==============================] - 20s 111ms/step - loss: 0.1084 - accuracy: 0.9631 - val_loss: 1.5365 - val_accuracy: 0.5619\n",
      "Epoch 108/500\n",
      "183/183 [==============================] - 21s 115ms/step - loss: 0.1105 - accuracy: 0.9643 - val_loss: 1.3646 - val_accuracy: 0.6416\n",
      "Epoch 109/500\n",
      "183/183 [==============================] - 20s 112ms/step - loss: 0.0852 - accuracy: 0.9698 - val_loss: 1.3735 - val_accuracy: 0.6150\n",
      "Epoch 110/500\n",
      "183/183 [==============================] - 21s 112ms/step - loss: 0.1119 - accuracy: 0.9676 - val_loss: 1.5271 - val_accuracy: 0.5885\n",
      "Epoch 111/500\n",
      "183/183 [==============================] - 20s 111ms/step - loss: 0.0838 - accuracy: 0.9745 - val_loss: 1.4523 - val_accuracy: 0.6018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "183/183 [==============================] - 20s 111ms/step - loss: 0.0931 - accuracy: 0.9716 - val_loss: 1.3770 - val_accuracy: 0.6283\n",
      "Epoch 113/500\n",
      "183/183 [==============================] - 21s 113ms/step - loss: 0.0964 - accuracy: 0.9655 - val_loss: 1.6755 - val_accuracy: 0.5885\n",
      "Epoch 114/500\n",
      "183/183 [==============================] - 21s 112ms/step - loss: 0.0970 - accuracy: 0.9662 - val_loss: 1.5963 - val_accuracy: 0.6239\n",
      "Epoch 115/500\n",
      "183/183 [==============================] - 21s 114ms/step - loss: 0.0874 - accuracy: 0.9713 - val_loss: 1.6306 - val_accuracy: 0.5752\n",
      "Epoch 116/500\n",
      "183/183 [==============================] - 22s 120ms/step - loss: 0.0907 - accuracy: 0.9700 - val_loss: 1.7704 - val_accuracy: 0.5619\n",
      "Epoch 117/500\n",
      "183/183 [==============================] - 20s 111ms/step - loss: 0.0864 - accuracy: 0.9713 - val_loss: 1.7266 - val_accuracy: 0.5929\n",
      "Epoch 118/500\n",
      "183/183 [==============================] - 20s 108ms/step - loss: 0.0757 - accuracy: 0.9763 - val_loss: 1.5072 - val_accuracy: 0.6283\n",
      "Epoch 119/500\n",
      "183/183 [==============================] - 19s 106ms/step - loss: 0.0759 - accuracy: 0.9764 - val_loss: 1.5058 - val_accuracy: 0.6239\n",
      "Epoch 120/500\n",
      "183/183 [==============================] - 20s 110ms/step - loss: 0.0755 - accuracy: 0.9744 - val_loss: 1.5233 - val_accuracy: 0.6106\n",
      "Epoch 121/500\n",
      "183/183 [==============================] - 19s 105ms/step - loss: 0.0891 - accuracy: 0.9693 - val_loss: 1.2976 - val_accuracy: 0.6416\n",
      "Epoch 122/500\n",
      "183/183 [==============================] - 20s 110ms/step - loss: 0.0803 - accuracy: 0.9728 - val_loss: 1.6254 - val_accuracy: 0.5708\n",
      "Epoch 123/500\n",
      "183/183 [==============================] - 20s 112ms/step - loss: 0.0861 - accuracy: 0.9710 - val_loss: 1.4773 - val_accuracy: 0.5929\n",
      "Epoch 124/500\n",
      "183/183 [==============================] - 19s 104ms/step - loss: 0.0739 - accuracy: 0.9758 - val_loss: 1.2849 - val_accuracy: 0.6593\n",
      "Epoch 125/500\n",
      "183/183 [==============================] - 20s 109ms/step - loss: 0.1021 - accuracy: 0.9667 - val_loss: 1.4378 - val_accuracy: 0.5973\n",
      "Epoch 126/500\n",
      "183/183 [==============================] - 20s 108ms/step - loss: 0.0984 - accuracy: 0.9693 - val_loss: 1.8130 - val_accuracy: 0.5929\n",
      "Epoch 127/500\n",
      "183/183 [==============================] - 19s 106ms/step - loss: 0.0775 - accuracy: 0.9739 - val_loss: 1.6858 - val_accuracy: 0.5885\n",
      "Epoch 128/500\n",
      "183/183 [==============================] - 19s 105ms/step - loss: 0.0639 - accuracy: 0.9789 - val_loss: 1.5950 - val_accuracy: 0.5796\n",
      "Epoch 129/500\n",
      "183/183 [==============================] - 19s 106ms/step - loss: 0.0676 - accuracy: 0.9798 - val_loss: 1.5305 - val_accuracy: 0.5885\n",
      "Epoch 130/500\n",
      "183/183 [==============================] - 21s 114ms/step - loss: 0.0708 - accuracy: 0.9788 - val_loss: 1.6730 - val_accuracy: 0.5841\n",
      "Epoch 131/500\n",
      "183/183 [==============================] - 20s 108ms/step - loss: 0.0697 - accuracy: 0.9760 - val_loss: 1.4960 - val_accuracy: 0.6195\n",
      "Epoch 132/500\n",
      "183/183 [==============================] - 20s 109ms/step - loss: 0.0689 - accuracy: 0.9747 - val_loss: 1.4495 - val_accuracy: 0.6150\n",
      "Epoch 133/500\n",
      "183/183 [==============================] - 21s 116ms/step - loss: 0.0723 - accuracy: 0.9748 - val_loss: 1.4141 - val_accuracy: 0.6106\n",
      "Epoch 134/500\n",
      "183/183 [==============================] - 21s 113ms/step - loss: 0.0683 - accuracy: 0.9762 - val_loss: 1.5092 - val_accuracy: 0.6195\n",
      "Epoch 135/500\n",
      "183/183 [==============================] - 19s 106ms/step - loss: 0.0679 - accuracy: 0.9758 - val_loss: 1.6772 - val_accuracy: 0.6018\n",
      "Epoch 136/500\n",
      "183/183 [==============================] - 21s 112ms/step - loss: 0.0751 - accuracy: 0.9776 - val_loss: 1.7883 - val_accuracy: 0.5575\n",
      "Epoch 137/500\n",
      "183/183 [==============================] - 19s 104ms/step - loss: 0.0658 - accuracy: 0.9786 - val_loss: 1.5652 - val_accuracy: 0.5796\n",
      "Epoch 138/500\n",
      "183/183 [==============================] - 17s 91ms/step - loss: 0.0719 - accuracy: 0.9799 - val_loss: 1.6604 - val_accuracy: 0.5619\n",
      "Epoch 139/500\n",
      "183/183 [==============================] - 17s 91ms/step - loss: 0.0776 - accuracy: 0.9772 - val_loss: 1.5280 - val_accuracy: 0.6106\n",
      "Epoch 140/500\n",
      "183/183 [==============================] - 17s 92ms/step - loss: 0.0755 - accuracy: 0.9723 - val_loss: 1.7430 - val_accuracy: 0.6018\n",
      "Epoch 141/500\n",
      "183/183 [==============================] - 17s 90ms/step - loss: 0.0655 - accuracy: 0.9776 - val_loss: 1.7800 - val_accuracy: 0.6283\n",
      "Epoch 142/500\n",
      "183/183 [==============================] - 16s 90ms/step - loss: 0.0905 - accuracy: 0.9684 - val_loss: 1.8178 - val_accuracy: 0.5221\n",
      "Epoch 143/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0705 - accuracy: 0.9761 - val_loss: 1.5485 - val_accuracy: 0.5708\n",
      "Epoch 144/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0632 - accuracy: 0.9756 - val_loss: 1.5973 - val_accuracy: 0.6150\n",
      "Epoch 145/500\n",
      "183/183 [==============================] - 16s 90ms/step - loss: 0.0697 - accuracy: 0.9762 - val_loss: 1.6962 - val_accuracy: 0.5664\n",
      "Epoch 146/500\n",
      "183/183 [==============================] - 16s 89ms/step - loss: 0.0617 - accuracy: 0.9795 - val_loss: 1.4630 - val_accuracy: 0.6416\n",
      "Epoch 147/500\n",
      "183/183 [==============================] - 17s 94ms/step - loss: 0.0674 - accuracy: 0.9776 - val_loss: 1.4575 - val_accuracy: 0.6195\n",
      "Epoch 148/500\n",
      "183/183 [==============================] - 17s 90ms/step - loss: 0.0732 - accuracy: 0.9761 - val_loss: 1.6942 - val_accuracy: 0.6018\n",
      "Epoch 149/500\n",
      "183/183 [==============================] - 16s 90ms/step - loss: 0.0762 - accuracy: 0.9757 - val_loss: 1.4621 - val_accuracy: 0.5841\n",
      "Epoch 150/500\n",
      "183/183 [==============================] - 16s 89ms/step - loss: 0.0656 - accuracy: 0.9763 - val_loss: 1.5808 - val_accuracy: 0.5752\n",
      "Epoch 151/500\n",
      "183/183 [==============================] - 16s 90ms/step - loss: 0.0772 - accuracy: 0.9753 - val_loss: 1.4185 - val_accuracy: 0.6549\n",
      "Epoch 152/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0757 - accuracy: 0.9724 - val_loss: 1.7302 - val_accuracy: 0.5708\n",
      "Epoch 153/500\n",
      "183/183 [==============================] - 17s 91ms/step - loss: 0.0570 - accuracy: 0.9816 - val_loss: 1.7467 - val_accuracy: 0.5664\n",
      "Epoch 154/500\n",
      "183/183 [==============================] - 17s 91ms/step - loss: 0.0851 - accuracy: 0.9723 - val_loss: 1.8048 - val_accuracy: 0.5619\n",
      "Epoch 155/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0592 - accuracy: 0.9779 - val_loss: 1.6437 - val_accuracy: 0.5973\n",
      "Epoch 156/500\n",
      "183/183 [==============================] - 17s 92ms/step - loss: 0.0570 - accuracy: 0.9787 - val_loss: 1.6893 - val_accuracy: 0.6018\n",
      "Epoch 157/500\n",
      "183/183 [==============================] - 16s 89ms/step - loss: 0.0779 - accuracy: 0.9724 - val_loss: 1.5042 - val_accuracy: 0.5885\n",
      "Epoch 158/500\n",
      "183/183 [==============================] - 16s 90ms/step - loss: 0.0576 - accuracy: 0.9796 - val_loss: 1.7550 - val_accuracy: 0.5973\n",
      "Epoch 159/500\n",
      "183/183 [==============================] - 17s 92ms/step - loss: 0.0715 - accuracy: 0.9754 - val_loss: 1.6136 - val_accuracy: 0.6106\n",
      "Epoch 160/500\n",
      "183/183 [==============================] - 17s 91ms/step - loss: 0.0651 - accuracy: 0.9767 - val_loss: 1.4981 - val_accuracy: 0.6283\n",
      "Epoch 161/500\n",
      "183/183 [==============================] - 17s 90ms/step - loss: 0.0661 - accuracy: 0.9792 - val_loss: 1.6926 - val_accuracy: 0.6460\n",
      "Epoch 162/500\n",
      "183/183 [==============================] - 17s 90ms/step - loss: 0.0606 - accuracy: 0.9777 - val_loss: 1.6112 - val_accuracy: 0.6416\n",
      "Epoch 163/500\n",
      "183/183 [==============================] - 17s 90ms/step - loss: 0.0457 - accuracy: 0.9860 - val_loss: 1.7675 - val_accuracy: 0.6239\n",
      "Epoch 164/500\n",
      "183/183 [==============================] - 17s 91ms/step - loss: 0.0474 - accuracy: 0.9827 - val_loss: 1.6389 - val_accuracy: 0.5929\n",
      "Epoch 165/500\n",
      "183/183 [==============================] - 16s 89ms/step - loss: 0.0600 - accuracy: 0.9798 - val_loss: 1.6395 - val_accuracy: 0.6018\n",
      "Epoch 166/500\n",
      "183/183 [==============================] - 17s 92ms/step - loss: 0.0629 - accuracy: 0.9794 - val_loss: 1.6298 - val_accuracy: 0.6018\n",
      "Epoch 167/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 17s 91ms/step - loss: 0.0609 - accuracy: 0.9782 - val_loss: 1.7338 - val_accuracy: 0.6195\n",
      "Epoch 168/500\n",
      "183/183 [==============================] - 16s 90ms/step - loss: 0.0573 - accuracy: 0.9811 - val_loss: 1.7346 - val_accuracy: 0.6150\n",
      "Epoch 169/500\n",
      "183/183 [==============================] - 16s 89ms/step - loss: 0.0595 - accuracy: 0.9803 - val_loss: 1.4739 - val_accuracy: 0.6195\n",
      "Epoch 170/500\n",
      "183/183 [==============================] - 17s 91ms/step - loss: 0.0488 - accuracy: 0.9821 - val_loss: 1.6521 - val_accuracy: 0.6106\n",
      "Epoch 171/500\n",
      "183/183 [==============================] - 17s 92ms/step - loss: 0.0515 - accuracy: 0.9824 - val_loss: 1.5748 - val_accuracy: 0.6018\n",
      "Epoch 172/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0522 - accuracy: 0.9813 - val_loss: 1.8330 - val_accuracy: 0.5929\n",
      "Epoch 173/500\n",
      "183/183 [==============================] - 16s 89ms/step - loss: 0.0695 - accuracy: 0.9777 - val_loss: 1.4069 - val_accuracy: 0.6549\n",
      "Epoch 174/500\n",
      "183/183 [==============================] - 17s 91ms/step - loss: 0.0593 - accuracy: 0.9783 - val_loss: 1.6567 - val_accuracy: 0.5885\n",
      "Epoch 175/500\n",
      "183/183 [==============================] - 17s 91ms/step - loss: 0.0499 - accuracy: 0.9827 - val_loss: 1.7661 - val_accuracy: 0.5796\n",
      "Epoch 176/500\n",
      "183/183 [==============================] - 17s 93ms/step - loss: 0.0621 - accuracy: 0.9782 - val_loss: 1.7944 - val_accuracy: 0.6195\n",
      "Epoch 177/500\n",
      "183/183 [==============================] - 17s 91ms/step - loss: 0.0518 - accuracy: 0.9853 - val_loss: 1.6021 - val_accuracy: 0.6239\n",
      "Epoch 178/500\n",
      "183/183 [==============================] - 17s 93ms/step - loss: 0.0509 - accuracy: 0.9836 - val_loss: 1.7184 - val_accuracy: 0.6150\n",
      "Epoch 179/500\n",
      "183/183 [==============================] - 16s 89ms/step - loss: 0.0559 - accuracy: 0.9806 - val_loss: 1.5652 - val_accuracy: 0.6150\n",
      "Epoch 180/500\n",
      "183/183 [==============================] - 17s 90ms/step - loss: 0.0605 - accuracy: 0.9808 - val_loss: 1.6753 - val_accuracy: 0.6283\n",
      "Epoch 181/500\n",
      "183/183 [==============================] - 16s 90ms/step - loss: 0.0487 - accuracy: 0.9855 - val_loss: 1.7761 - val_accuracy: 0.6195\n",
      "Epoch 182/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0545 - accuracy: 0.9819 - val_loss: 1.7798 - val_accuracy: 0.5929\n",
      "Epoch 183/500\n",
      "183/183 [==============================] - 17s 91ms/step - loss: 0.0473 - accuracy: 0.9826 - val_loss: 1.6901 - val_accuracy: 0.5929\n",
      "Epoch 184/500\n",
      "183/183 [==============================] - 17s 92ms/step - loss: 0.0517 - accuracy: 0.9834 - val_loss: 1.6922 - val_accuracy: 0.6018\n",
      "Epoch 185/500\n",
      "183/183 [==============================] - 16s 89ms/step - loss: 0.0605 - accuracy: 0.9807 - val_loss: 1.7277 - val_accuracy: 0.6372\n",
      "Epoch 186/500\n",
      "183/183 [==============================] - 16s 89ms/step - loss: 0.0572 - accuracy: 0.9797 - val_loss: 1.7397 - val_accuracy: 0.6018\n",
      "Epoch 187/500\n",
      "183/183 [==============================] - 16s 89ms/step - loss: 0.0464 - accuracy: 0.9847 - val_loss: 1.7193 - val_accuracy: 0.5664\n",
      "Epoch 188/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0540 - accuracy: 0.9820 - val_loss: 1.6558 - val_accuracy: 0.5885\n",
      "Epoch 189/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0466 - accuracy: 0.9842 - val_loss: 1.7041 - val_accuracy: 0.5619\n",
      "Epoch 190/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0484 - accuracy: 0.9852 - val_loss: 1.8337 - val_accuracy: 0.5929\n",
      "Epoch 191/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0488 - accuracy: 0.9857 - val_loss: 1.6761 - val_accuracy: 0.5487\n",
      "Epoch 192/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0591 - accuracy: 0.9811 - val_loss: 1.4116 - val_accuracy: 0.6239\n",
      "Epoch 193/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0451 - accuracy: 0.9866 - val_loss: 1.5146 - val_accuracy: 0.6283\n",
      "Epoch 194/500\n",
      "183/183 [==============================] - 16s 89ms/step - loss: 0.0335 - accuracy: 0.9878 - val_loss: 1.7422 - val_accuracy: 0.5973\n",
      "Epoch 195/500\n",
      "183/183 [==============================] - 16s 89ms/step - loss: 0.0500 - accuracy: 0.9806 - val_loss: 1.7356 - val_accuracy: 0.5973\n",
      "Epoch 196/500\n",
      "183/183 [==============================] - 16s 89ms/step - loss: 0.0493 - accuracy: 0.9840 - val_loss: 1.7748 - val_accuracy: 0.6018\n",
      "Epoch 197/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0348 - accuracy: 0.9886 - val_loss: 1.7212 - val_accuracy: 0.5973\n",
      "Epoch 198/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0580 - accuracy: 0.9820 - val_loss: 1.5182 - val_accuracy: 0.6549\n",
      "Epoch 199/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0600 - accuracy: 0.9811 - val_loss: 1.5403 - val_accuracy: 0.6327\n",
      "Epoch 200/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0416 - accuracy: 0.9865 - val_loss: 1.8472 - val_accuracy: 0.5752\n",
      "Epoch 201/500\n",
      "183/183 [==============================] - 17s 92ms/step - loss: 0.0519 - accuracy: 0.9817 - val_loss: 1.5574 - val_accuracy: 0.6018\n",
      "Epoch 202/500\n",
      "183/183 [==============================] - 17s 91ms/step - loss: 0.0561 - accuracy: 0.9833 - val_loss: 1.4334 - val_accuracy: 0.6416\n",
      "Epoch 203/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0479 - accuracy: 0.9845 - val_loss: 1.6317 - val_accuracy: 0.6150\n",
      "Epoch 204/500\n",
      "183/183 [==============================] - 16s 89ms/step - loss: 0.0563 - accuracy: 0.9812 - val_loss: 1.5102 - val_accuracy: 0.6106\n",
      "Epoch 205/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0381 - accuracy: 0.9861 - val_loss: 1.6527 - val_accuracy: 0.5796\n",
      "Epoch 206/500\n",
      "183/183 [==============================] - 16s 87ms/step - loss: 0.0491 - accuracy: 0.9838 - val_loss: 1.6471 - val_accuracy: 0.5664\n",
      "Epoch 207/500\n",
      "183/183 [==============================] - 17s 90ms/step - loss: 0.0607 - accuracy: 0.9840 - val_loss: 1.6800 - val_accuracy: 0.5841\n",
      "Epoch 208/500\n",
      "183/183 [==============================] - 17s 90ms/step - loss: 0.0533 - accuracy: 0.9794 - val_loss: 1.5832 - val_accuracy: 0.6062\n",
      "Epoch 209/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0311 - accuracy: 0.9894 - val_loss: 1.7244 - val_accuracy: 0.5796\n",
      "Epoch 210/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0527 - accuracy: 0.9816 - val_loss: 1.7632 - val_accuracy: 0.5619\n",
      "Epoch 211/500\n",
      "183/183 [==============================] - 17s 90ms/step - loss: 0.0455 - accuracy: 0.9841 - val_loss: 1.6937 - val_accuracy: 0.6150\n",
      "Epoch 212/500\n",
      "183/183 [==============================] - 17s 91ms/step - loss: 0.0424 - accuracy: 0.9862 - val_loss: 1.8318 - val_accuracy: 0.5664\n",
      "Epoch 213/500\n",
      "183/183 [==============================] - 17s 91ms/step - loss: 0.0455 - accuracy: 0.9850 - val_loss: 1.7827 - val_accuracy: 0.5973\n",
      "Epoch 214/500\n",
      "183/183 [==============================] - 16s 88ms/step - loss: 0.0524 - accuracy: 0.9837 - val_loss: 1.6024 - val_accuracy: 0.5796\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00214: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fit model\n",
    "history = cnn_model.fit(training_set,\n",
    "        epochs=500,\n",
    "        validation_data=val_set,\n",
    "        verbose = 1, \n",
    "        validation_steps = len(val_set),\n",
    "        callbacks = [es])\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = cnn_model.evaluate(training_set, verbose=1)\n",
    "_, val_acc = cnn_model.evaluate(val_set, verbose=1)\n",
    "print('Train: %.3f, Val: %.3f' % (train_acc, val_acc))\n",
    "\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "cnn_model.save(\"model_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, None, None, 32)    896       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, None, None, 32)    128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, None, None, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, None, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, None, None, 64)    18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, None, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, None, 128)   512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, None, None, 128)   512       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, None, None, 32)    65568     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, None, None, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, None, None, 64)    18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, None, None, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, None, None, 128)   512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, None, None, 128)   512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, None, None, 128)   262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, None, None, 128)   512       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, None, None, 128)   512       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, None, None, 128)   512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, None)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 914,597\n",
      "Trainable params: 911,589\n",
      "Non-trainable params: 3,008\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model Summary\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/229 [==============================] - 1s 6ms/step - loss: 1.6571 - accuracy: 0.5459\n",
      "Test loss: 1.6571236848831177\n",
      "Test accuracy: 0.5458515286445618\n"
     ]
    }
   ],
   "source": [
    "#test performance\n",
    "\n",
    "test_loss, test_acc = cnn_model.evaluate(test_set)\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions\n",
    "\n",
    "Code still needs improvement\n",
    "\n",
    "These predictions are gated by batch size\n",
    "\n",
    "Let's try and improve it with bruteforce Hyperparam optimization whiole being on the lookout for other ways to improve performance. Model is predicting all as Amorphous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-2d09e246d562>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "#get predicted probality\n",
    "prediction = cnn_model.predict_generator(test_set,verbose=1)\n",
    "\n",
    "#Get class of prediction\n",
    "predicted_class = np.argmax(prediction,axis=1)\n",
    "\n",
    "# get names of pictures\n",
    "filenames = test_set.filenames\n",
    "\n",
    "#store info in dataframe\n",
    "robust_predictions = pd.DataFrame({'Filename': filenames,'Model_1': prednames})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join prediction to dataframe\n",
    "df_predictions = df_predictions.merge(robust_predictions, on = 'Filename', how = 'inner')\n",
    "\n",
    "del robust_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 - Benchmarks and other tests\n",
    "\n",
    "Try and compare the models performance with other standardly used models:\n",
    "\n",
    "AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALEX NET\n",
    "\n",
    "#Importing library \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "#Instantiation\n",
    "AlexNet = Sequential()\n",
    "\n",
    "#1st Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=96, input_shape=(32,32,3), kernel_size=(11,11), strides=(4,4), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#2nd Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#3rd Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "#4th Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "#5th Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#Passing it to a Fully Connected layer\n",
    "AlexNet.add(Flatten())\n",
    "# 1st Fully Connected Layer\n",
    "AlexNet.add(Dense(4096, input_shape=(32,32,3,)))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "#2nd Fully Connected Layer\n",
    "AlexNet.add(Dense(4096))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "#Add Dropout\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "#3rd Fully Connected Layer\n",
    "AlexNet.add(Dense(1000))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "#Add Dropout\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "#Output Layer\n",
    "AlexNet.add(Dense(5))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('softmax'))\n",
    "\n",
    "#Model Summary\n",
    "AlexNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "AlexNet.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "\n",
    "history = AlexNet.fit(training_set,\n",
    "        epochs=500,\n",
    "        validation_data=val_set,\n",
    "        verbose = 1, \n",
    "        validation_steps = len(val_set),\n",
    "        callbacks = [es])\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = cnn_model.evaluate(training_set, verbose=1)\n",
    "_, val_acc = cnn_model.evaluate(val_set, verbose=1)\n",
    "print('Train: %.3f, Val: %.3f' % (train_acc, val_acc))\n",
    "\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = AlexNet.evaluate(test_set)\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted probality\n",
    "prediction = AlexNet.predict_generator(test_set,verbose=1)\n",
    "\n",
    "#Get class of prediction\n",
    "predicted_class = np.argmax(prediction,axis=1)\n",
    "\n",
    "# get names of pictures\n",
    "filenames = test_set.filenames\n",
    "\n",
    "#store info in dataframe\n",
    "alex_predictions = pd.DataFrame({'Filename': filenames,'AlexNet': prednames})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join prediction to dataframe\n",
    "df_predictions = df_predictions.merge(alex_predictions, on = 'Filename', how = 'inner')\n",
    "\n",
    "del alex_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning with RESNET50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet50 requires its own preprocessing \n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "#to play around with these\n",
    "res50train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   preprocessing_function=preprocess_input,\n",
    "                                   rotation_range = 5,\n",
    "                                   width_shift_range = 0.06, \n",
    "                                   height_shift_range = 0.06, \n",
    "                                   vertical_flip = True,\n",
    "                                   horizontal_flip = True,\n",
    "                                   shuffle = True,\n",
    "                                   brightness_range=[0.2,1.2], \n",
    "                                   fill_mode='nearest',\n",
    "                                   zoom_range = 0.2,\n",
    "                                   ) \n",
    "\n",
    "res50val_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function=preprocess_input)\n",
    "res50test_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function=preprocess_input)\n",
    "\n",
    "#test different color maps -  class modes and cross validation types\n",
    "res_train = res50train_datagen.flow_from_directory(path+'/train',\n",
    "                                                 target_size = (32, 32),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode=\"categorical\",\n",
    "                                                 color_mode = 'rgb',\n",
    "                                                 shuffle = True)\n",
    "\n",
    "res_val = res50val_datagen.flow_from_directory(path+'/val',\n",
    "                                            target_size = (32, 32),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            color_mode = 'rgb',\n",
    "                                            shuffle = True)\n",
    "\n",
    "res_test = res50test_datagen.flow_from_directory(path+'/test',\n",
    "                                            target_size = (32, 32),\n",
    "                                            batch_size = 1,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            color_mode = 'rgb',\n",
    "                                            shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "\n",
    "res50model = Sequential()\n",
    "\n",
    "#Layer 1: RES50 without top layer\n",
    "res50model.add(ResNet50(weights='imagenet', input_shape= (32,32,3),\n",
    "                 include_top = False, classes=5,))\n",
    "\n",
    "#Layer 2: RES50 without top layer\n",
    "res50model.add(Flatten())\n",
    "res50model.add(Dense(5, activation = 'softmax'))\n",
    "\n",
    "#freeze layers in resnet - weights obtained with IMAGENET challenge, we only train final layer\n",
    "res50model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile\n",
    "\n",
    "res50model.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "\n",
    "res_history = res50model.fit(res_train,\n",
    "        epochs=500,\n",
    "        validation_data=res_val,\n",
    "        verbose = 1, \n",
    "        validation_steps = len(val_set),\n",
    "        callbacks = [es])\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = cnn_model.evaluate(training_set, verbose=1)\n",
    "_, val_acc = cnn_model.evaluate(val_set, verbose=1)\n",
    "print('Train: %.3f, Val: %.3f' % (train_acc, val_acc))\n",
    "\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calling `save('res50_net.h5')`.\n",
    "res50model.save(\"Res50_net.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = res50model.evaluate(res_test)\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted probality\n",
    "prediction = res50model.predict_generator(res_test,verbose=1)\n",
    "\n",
    "#Get class of prediction\n",
    "predicted_class = np.argmax(prediction,axis=1)\n",
    "\n",
    "# get names of pictures\n",
    "filenames = res_test.filenames\n",
    "\n",
    "#store info in dataframe\n",
    "res50_prebuilt_predictions = pd.DataFrame({'Filename': filenames,'ResNet50': prednames})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join prediction to dataframe\n",
    "df_predictions = df_predictions.merge(res50_prebuilt_predictions, on = 'Filename', how = 'inner')\n",
    "\n",
    "del res50_prebuilt_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESNET50 Trained from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50model_2 = Sequential()\n",
    "\n",
    "#Layer 1: RES50 without top layer\n",
    "res50model_2.add(ResNet50(weights=None, input_shape= (32,32,3),\n",
    "                 include_top = False, classes=5,))\n",
    "\n",
    "#Layer 2: RES50 without top layer\n",
    "res50model_2.add(Flatten())\n",
    "res50model_2.add(Dense(5, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile\n",
    "\n",
    "res50model_2.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "\n",
    "res_history_2 = res50model_2.fit(res_train,\n",
    "        epochs=500,\n",
    "        validation_data=res_val,\n",
    "        verbose = 1, \n",
    "        validation_steps = len(res_val),\n",
    "        callbacks = [es])\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = cnn_model.evaluate(res_train, verbose=1)\n",
    "_, val_acc = cnn_model.evaluate(res_val, verbose=1)\n",
    "print('Train: %.3f, Val: %.3f' % (train_acc, val_acc))\n",
    "\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calling `save('my_model.h5')` creates a h5 file `my_model.h5`.\n",
    "res50model.save(\"Res50_from_scratch.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance do primeiro teste\n",
    "\n",
    "test_loss, test_acc = res50model_2.evaluate(res_test)\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted probality\n",
    "prediction = res50model_2.predict_generator(res_test,verbose=1)\n",
    "\n",
    "#Get class of prediction\n",
    "predicted_class = np.argmax(prediction,axis=1)\n",
    "\n",
    "# get names of pictures\n",
    "filenames = res_test.filenames\n",
    "\n",
    "#store info in dataframe\n",
    "res50_predictions = pd.DataFrame({'Filename': filenames,'ResNet50 From Scratch': prednames})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join prediction to dataframe\n",
    "df_predictions = df_predictions.merge(res50_predictions, on = 'Filename', how = 'inner')\n",
    "\n",
    "del res50_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR TOMORROW\n",
    "\n",
    "Check different loss functions\n",
    "CROSS VALIDATION\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
