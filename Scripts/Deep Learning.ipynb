{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project\n",
    "\n",
    "The current project relies on using CNNs in order to process sperm data. The goal is to develop a classifier able to identify sperm cells.\n",
    "\n",
    "The model is not yet fuly defined. We will start with a standard CNN to Dense Layer.\n",
    "\n",
    "The goal is for images to be loaded into the CNN. The CNN will will then perform feature extraction and those will be fed to the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Create the pipeline\n",
    "\n",
    "The Deep learning model will be made out of 2 different parts: \n",
    "\n",
    "1. A CNN that takes the images as inputs and performs feature extraction.\n",
    "2. A dense, fully connected layer that will perform classification itself with the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12838017832935611584\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 8080208073006711878\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4971491488\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16020402108907825257\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 2137664995782886590\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#need this to run, don't know why\n",
    "import os\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "#Check Computer's available devices\n",
    "#Will need to check in the future\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "#cfg 1\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "#cfg2\n",
    "config = tf.compat.v1.ConfigProto(gpu_options = \n",
    "                         tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "# device_count = {'GPU': 1}\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import some libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "#image manipulation packages\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "#Data Augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#Classification\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix\n",
    "\n",
    "sns.set()\n",
    "#import keras\n",
    "from tensorflow import keras\n",
    "\n",
    "import glob\n",
    "import cv2\n",
    "import shutil\n",
    "import random\n",
    "import Augmentor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Label images\n",
    "\n",
    "In this case, we have a folder with 1132 image - SCIAN-MorphoSpermGS folder - https://cimt.uchile.cl/gold10/. Each image is 35 x 35 pixels and has been classified by 3 experts. We will use majority vote result as target. Each image will need to be loaded and the dataset will need to be created.\n",
    "\n",
    "#### This will yield a dataset with the picture name and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the path where you've put your dataset provided in Moodle\n",
    "\n",
    "#step 1: change directory back\n",
    "os.chdir('../Dataset')\n",
    "path = os.getcwd()\n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p1-pl2-sample01/Sperm_01</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p1-pl2-sample01/Sperm_02</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p1-pl2-sample01/Sperm_03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p1-pl2-sample01/Sperm_04</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p1-pl2-sample01/Sperm_05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>p5-pl1-sample20/Sperm_07</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>p5-pl1-sample20/Sperm_09</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>p5-pl1-sample20/Sperm_11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>p5-pl1-sample20/Sperm_12</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>p5-pl1-sample20/Sperm_13</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1132 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0  1  2  3  4\n",
       "0     p1-pl2-sample01/Sperm_01  5  5  5  5\n",
       "1     p1-pl2-sample01/Sperm_02  1  5  5  5\n",
       "2     p1-pl2-sample01/Sperm_03  0  0  0  0\n",
       "3     p1-pl2-sample01/Sperm_04  1  5  5  5\n",
       "4     p1-pl2-sample01/Sperm_05  0  0  0  0\n",
       "...                        ... .. .. .. ..\n",
       "1127  p5-pl1-sample20/Sperm_07  1  5  5  5\n",
       "1128  p5-pl1-sample20/Sperm_09  1  1  5  1\n",
       "1129  p5-pl1-sample20/Sperm_11  1  1  5  1\n",
       "1130  p5-pl1-sample20/Sperm_12  5  5  5  5\n",
       "1131  p5-pl1-sample20/Sperm_13  1  5  5  5\n",
       "\n",
       "[1132 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 - load txt file\n",
    "dataframe = pd.read_csv(path + '\\PA-expert-annotations.txt', sep = '\\\\t', header = None, engine = 'python')\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Start by most standard preprocessing of Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-6bf5cf9e776b>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['Majority_Vote'] = dataframe['Majority_Vote'].replace(5, 3)\n",
      "<ipython-input-5-6bf5cf9e776b>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['Sperm_Pic'] = 'ch00_' + dataframe['Sperm_Pic'] + '.tif'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sperm_Pic</th>\n",
       "      <th>Majority_Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ch00_p1-pl2-sample01-sperm_01.tif</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ch00_p1-pl2-sample01-sperm_02.tif</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ch00_p1-pl2-sample01-sperm_03.tif</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ch00_p1-pl2-sample01-sperm_04.tif</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ch00_p1-pl2-sample01-sperm_05.tif</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>ch00_p5-pl1-sample20-sperm_07.tif</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>ch00_p5-pl1-sample20-sperm_09.tif</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>ch00_p5-pl1-sample20-sperm_11.tif</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>ch00_p5-pl1-sample20-sperm_12.tif</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>ch00_p5-pl1-sample20-sperm_13.tif</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1060 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Sperm_Pic  Majority_Vote\n",
       "0     ch00_p1-pl2-sample01-sperm_01.tif              3\n",
       "1     ch00_p1-pl2-sample01-sperm_02.tif              3\n",
       "2     ch00_p1-pl2-sample01-sperm_03.tif              0\n",
       "3     ch00_p1-pl2-sample01-sperm_04.tif              3\n",
       "4     ch00_p1-pl2-sample01-sperm_05.tif              0\n",
       "...                                 ...            ...\n",
       "1127  ch00_p5-pl1-sample20-sperm_07.tif              3\n",
       "1128  ch00_p5-pl1-sample20-sperm_09.tif              1\n",
       "1129  ch00_p5-pl1-sample20-sperm_11.tif              1\n",
       "1130  ch00_p5-pl1-sample20-sperm_12.tif              3\n",
       "1131  ch00_p5-pl1-sample20-sperm_13.tif              3\n",
       "\n",
       "[1060 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#editions to target dataset\n",
    "dataframe.rename(columns={0 :\"Sperm_Pic\", 1: 'Expert_1', 2: 'Expert_2', 3: 'Expert_3', 4: 'Majority_Vote'}, inplace = True)\n",
    "dataframe['Sperm_Pic'] = dataframe['Sperm_Pic'].str.replace('/S', '-s')\n",
    "\n",
    "#delete tiny\n",
    "dataframe = dataframe.loc[dataframe['Majority_Vote'] != 3]\n",
    "dataframe['Majority_Vote'] = dataframe['Majority_Vote'].replace(5, 3)\n",
    "\n",
    "\n",
    "#add ch_00 to all rows:\n",
    "dataframe['Sperm_Pic'] = 'ch00_' + dataframe['Sperm_Pic'] + '.tif'\n",
    "\n",
    "#Drop all irrelevant features\n",
    "dataframe = dataframe.drop(['Expert_1', 'Expert_2', 'Expert_3'], axis = 1)\n",
    "\n",
    "#Show\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Place images on Folder based on image Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Partial-Agreement-Images')\n",
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-bb3cf003d795>:40: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if maj_vote.values == 3:\n",
      "<ipython-input-8-bb3cf003d795>:42: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if maj_vote.values == 2:\n",
      "<ipython-input-8-bb3cf003d795>:44: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if maj_vote.values == 1:\n",
      "<ipython-input-8-bb3cf003d795>:46: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if maj_vote.values == 0:\n"
     ]
    }
   ],
   "source": [
    "imgs = []\n",
    "\n",
    "for image in glob.glob(path+'/*.tif'):\n",
    "        \n",
    "    imgs.append(cv2.imread(image))\n",
    "    \n",
    "    #names of each sample in the image and dataframe are different, gotta deal with this here to match them\n",
    "\n",
    "    img_name = image.split('\\\\')[-1]\n",
    "    check_length = img_name.split('-')[-2]\n",
    "    if len(check_length) == 7:\n",
    "        img_name = img_name[:18] + '0' + img_name[18:]\n",
    "    check_length = img_name.split('-')[-1]\n",
    "    if len(check_length) == 10:\n",
    "        img_name = img_name[:-5] + '0' + img_name[-5:]\n",
    "    img_name = img_name[:-6] + '_' + img_name[-6:]\n",
    "   \n",
    "    #path of each class\n",
    "    path0 = path+'\\\\class0'\n",
    "    path1 = path+'\\\\class1'\n",
    "    path2 = path+'\\\\class2'\n",
    "    path3 = path+'\\\\class3'\n",
    "   \n",
    "    #creates folders to store images from eacg category, if not exists already\n",
    "    #requires dirs to exist - otherwise enters infinite loop and we have to restrat script\n",
    "    if not os.path.isdir(path0):\n",
    "\n",
    "        path = path3\n",
    "        os.mkdir(path)\n",
    "        path = path2\n",
    "        os.mkdir(path)\n",
    "        path = path1\n",
    "        os.mkdir(path)\n",
    "        path = path0\n",
    "        os.mkdir(path)\n",
    "\n",
    "    #creates copy of image based on label\n",
    "    maj_vote = dataframe[dataframe['Sperm_Pic'].str.contains(img_name)]['Majority_Vote']\n",
    "\n",
    "    if maj_vote.values == 3:\n",
    "        shutil.copy(image, path3)\n",
    "    if maj_vote.values == 2:\n",
    "        shutil.copy(image, path2)\n",
    "    if maj_vote.values == 1:\n",
    "        shutil.copy(image, path1)\n",
    "    if maj_vote.values == 0:\n",
    "        shutil.copy(image, path0)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    656\n",
       "1    228\n",
       "0    100\n",
       "2     76\n",
       "Name: Majority_Vote, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#comparing to images on each folder\n",
    "dataframe['Majority_Vote'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_001.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_002.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_003.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_004.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_005.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_006.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_007.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_008.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_009.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_010.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_011.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_012.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_013.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_014.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_015.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_016.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_017.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_018.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_019.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_020.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_021.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_022.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_023.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_024.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_025.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_026.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_027.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_028.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_029.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_030.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_031.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_032.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_033.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_034.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_035.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_036.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_037.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_038.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_039.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_040.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_041.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_042.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_043.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_044.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_045.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_046.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_047.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_048.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_049.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_050.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_051.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_052.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_053.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class0\\\\image_054.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_001.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_002.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_003.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_004.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_005.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_006.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_007.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_008.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_009.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_010.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_011.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_012.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_013.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_014.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_015.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_016.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_017.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_018.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_019.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_020.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_021.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_022.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_023.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_024.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_025.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_026.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_027.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_028.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_029.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_030.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_031.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_032.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_033.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_034.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_035.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_036.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_037.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_038.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_039.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_040.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_041.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_042.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_043.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_044.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_045.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_046.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_047.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_048.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_049.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_050.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_051.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_052.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class1\\\\image_053.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_001.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_002.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_003.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_004.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_005.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_006.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_007.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_008.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_009.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_010.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_011.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_012.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_013.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_014.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_015.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_016.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_017.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_018.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_019.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_020.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_021.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_022.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_023.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_024.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_025.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_026.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_027.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_028.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_029.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_030.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_031.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_032.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_033.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_034.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_035.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_036.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_037.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_038.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_039.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_040.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_041.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_042.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_043.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_044.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_045.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_046.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_047.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_048.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_049.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_050.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_051.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_052.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_053.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_054.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_055.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_056.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class2\\\\image_057.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_001.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_002.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_003.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_004.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_005.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_006.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_007.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_008.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_009.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_010.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_011.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_012.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_013.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_014.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_015.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_016.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_017.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_018.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_019.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_020.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_021.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_022.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_023.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_024.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_025.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_026.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_027.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_028.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_029.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_030.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_031.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_032.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_033.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_034.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_035.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_036.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_037.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_038.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_039.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_040.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_041.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_042.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_043.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_044.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_045.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_046.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_047.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_048.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_049.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_050.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_051.BMP',\n",
       " 'C:\\\\Users\\\\Ricardo Santos\\\\Desktop\\\\Mestrado Ricardo\\\\Ano 1\\\\Spring Semester\\\\Deep\\\\Deep_Learning_Project\\\\Dataset\\\\Partial-Agreement-Images\\\\class3\\\\image_052.BMP']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Optional Join Hushem Data\n",
    "from distutils.dir_util import copy_tree\n",
    "\n",
    "# copy subdirectory example\n",
    "fromDirectory = r\"C:\\Users\\Ricardo Santos\\Desktop\\Mestrado Ricardo\\Ano 1\\Spring Semester\\Deep\\Deep_Learning_Project\\Dataset\\HuSHem\"\n",
    "toDirectory = r\"C:\\Users\\Ricardo Santos\\Desktop\\Mestrado Ricardo\\Ano 1\\Spring Semester\\Deep\\Deep_Learning_Project\\Dataset\\Partial-Agreement-Images\"\n",
    "\n",
    "copy_tree(fromDirectory, toDirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Create Folders for Training, Validation and Test Data\n",
    "The Current solution is somewhat innefficient because it requires the creation of 2 copies of image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_dir = ['/class0', '/class1', '/class2', '/class3']\n",
    "\n",
    "val_ratio = 0.20\n",
    "test_ratio = 0.20\n",
    "\n",
    "if not os.path.isdir(path + '/train'):\n",
    "\n",
    "    for cls in classes_dir:\n",
    "    \n",
    "        #creates train and test folders, with each class separated inside\n",
    "        os.makedirs(path +'/train' + cls)\n",
    "        os.makedirs(path +'/val' + cls)\n",
    "        os.makedirs(path +'/test' + cls)\n",
    "\n",
    "\n",
    "        # Creating partitions of the data after shuffeling\n",
    "        src = path + cls # Folder to copy images from\n",
    "\n",
    "        allFileNames = os.listdir(src)\n",
    "        np.random.shuffle(allFileNames)\n",
    "        \n",
    "        train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),\n",
    "                                                              [int(len(allFileNames)* (1 - val_ratio - test_ratio)), \n",
    "                                                               int(len(allFileNames)* (1 - val_ratio))])\n",
    "        \n",
    "        train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]\n",
    "        val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]\n",
    "        test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]\n",
    "\n",
    "    \n",
    "        # Copy-pasting images\n",
    "        for name in train_FileNames:\n",
    "            shutil.copy(name, path +'/train' + cls)\n",
    "\n",
    "        for name in val_FileNames:\n",
    "            shutil.copy(name, path +'/val' + cls)\n",
    "\n",
    "        for name in test_FileNames:\n",
    "            shutil.copy(name, path +'/test' + cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images Normal: 92\n",
      "Images Tapered: 168\n",
      "Images Pyriform: 79\n",
      "Images Amorphous: 424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#count files in folder\n",
    "\n",
    "def fileCount(folder):\n",
    "    \"count the number of files in a directory\"\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        path = os.path.join(folder, filename)\n",
    "\n",
    "        if os.path.isfile(path):\n",
    "            count += 1\n",
    "        elif os.path.isdir(path):\n",
    "            count += fileCount(path)\n",
    "\n",
    "    return count\n",
    "\n",
    "count_0 = fileCount(path+'/train/class0')\n",
    "count_1 = fileCount(path+'/train/class1')\n",
    "count_2 = fileCount(path+'/train/class2')\n",
    "count_3 = fileCount(path+'/train/class3')\n",
    "\n",
    "print(f'Images Normal: {count_0}\\n' +\n",
    "     f'Images Tapered: {count_1}\\n' +\n",
    "     f'Images Pyriform: {count_2}\\n' +\n",
    "     f'Images Amorphous: {count_3}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Augment images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 92 image(s) found.\n",
      "Output directory set to C:\\Users\\Ricardo Santos\\Desktop\\Mestrado Ricardo\\Ano 1\\Spring Semester\\Deep\\Deep_Learning_Project\\Dataset\\Partial-Agreement-Images/train/class0\\output.Initialised with 168 image(s) found.\n",
      "Output directory set to C:\\Users\\Ricardo Santos\\Desktop\\Mestrado Ricardo\\Ano 1\\Spring Semester\\Deep\\Deep_Learning_Project\\Dataset\\Partial-Agreement-Images/train/class1\\output.Initialised with 79 image(s) found.\n",
      "Output directory set to C:\\Users\\Ricardo Santos\\Desktop\\Mestrado Ricardo\\Ano 1\\Spring Semester\\Deep\\Deep_Learning_Project\\Dataset\\Partial-Agreement-Images/train/class2\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=131x131 at 0x2DAE54128E0>:   2%|█                                           | 23/932 [00:00<00:09, 99.02 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 424 image(s) found.\n",
      "Output directory set to C:\\Users\\Ricardo Santos\\Desktop\\Mestrado Ricardo\\Ano 1\\Spring Semester\\Deep\\Deep_Learning_Project\\Dataset\\Partial-Agreement-Images/train/class3\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=L size=34x34 at 0x2DAE5319190>: 100%|██████████████████████████████████████████████| 932/932 [00:01<00:00, 542.18 Samples/s]\n",
      "Processing <PIL.Image.Image image mode=L size=35x35 at 0x2DAE7B1E040>: 100%|██████████████████████████████████████████████| 856/856 [00:01<00:00, 542.11 Samples/s]\n",
      "Processing <PIL.Image.Image image mode=RGB size=131x131 at 0x2DAE5696490>: 100%|██████████████████████████████████████████| 945/945 [00:01<00:00, 569.28 Samples/s]\n",
      "Processing <PIL.Image.Image image mode=L size=35x35 at 0x2DAE53190A0>: 100%|██████████████████████████████████████████████| 600/600 [00:01<00:00, 518.58 Samples/s]\n"
     ]
    }
   ],
   "source": [
    "# Define augmentation pipelines\n",
    "class_0 = Augmentor.Pipeline(path+'/train/class0')\n",
    "class_1 = Augmentor.Pipeline(path+'/train/class1')\n",
    "class_2 = Augmentor.Pipeline(path+'/train/class2')\n",
    "class_3 = Augmentor.Pipeline(path+'/train/class3')\n",
    "\n",
    "# Define different augmentations depending on the pipeline; options are limited since we're working with microscopy data\n",
    "\n",
    "#visit: https://augmentor.readthedocs.io/en/master/userguide/mainfeatures.html#rotating\n",
    "#options are:\n",
    "#Perspective skewing: does not make sense; microscopy data\n",
    "###Elastic distortion: could work, but I'm afraid it'll actually deform the shape of the sper cell, which is what is picked up to classify it; could be detremental...\n",
    "###Rotation: makes sense! But no more than 5 degrees left or right...\n",
    "#Shear: does not make sense?\n",
    "#Cropping: does not make sense.\n",
    "###Mirroring: yes! both vertically and horizontally, randomly.\n",
    "\n",
    "\n",
    "#rotate by a maximum of 5 degrees\n",
    "class_0.rotate(probability=0.7, max_left_rotation=5, max_right_rotation=5)\n",
    "class_1.rotate(probability=0.7, max_left_rotation=5, max_right_rotation=5)\n",
    "class_2.rotate(probability=0.7, max_left_rotation=5, max_right_rotation=5)\n",
    "class_3.rotate(probability=0.7, max_left_rotation=5, max_right_rotation=5)\n",
    "\n",
    "#mirroring, vertical or horizontal, randomly\n",
    "class_0.flip_random(probability=0.7)\n",
    "class_1.flip_random(probability=0.7)\n",
    "class_2.flip_random(probability=0.7)\n",
    "class_3.flip_random(probability=0.7)\n",
    "\n",
    "\n",
    "# Augment images to the same proportion as existing ones in class 4 (majority class - get to 1000 in each class)\n",
    "class_0.sample(1024- count_0)\n",
    "class_1.sample(1024- count_1)\n",
    "class_2.sample(1024- count_2)\n",
    "class_3.sample(1024- count_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4096 images belonging to 4 classes.\n",
      "Found 256 images belonging to 4 classes.\n",
      "Found 257 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "#load image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#to play around with these\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   rotation_range = 5, #rotates images from -5 to 5 degrees\n",
    "                                   width_shift_range = 0.06, #translates images by 6% to left or right\n",
    "                                   height_shift_range = 0.06, #translates images by 6% up and down\n",
    "                                   vertical_flip = True,\n",
    "                                   horizontal_flip = True,\n",
    "                                   brightness_range=[0.2,1.2], \n",
    "                                   fill_mode='nearest',\n",
    "                                   zoom_range = 0.2,\n",
    "                                   ) \n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "#test different color maps -  class modes and cross validation types\n",
    "training_set = train_datagen.flow_from_directory(path+'/train',\n",
    "                                                 target_size = (32, 32),\n",
    "                                                 batch_size = 32,\n",
    "                                                 shuffle = True,\n",
    "                                                 class_mode=\"categorical\",\n",
    "                                                 color_mode = 'rgb')\n",
    "\n",
    "val_set = val_datagen.flow_from_directory(path+'/val',\n",
    "                                            target_size = (32, 32),\n",
    "                                            batch_size = 32,\n",
    "                                            shuffle = True,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            color_mode = 'rgb')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(path+'/test',\n",
    "                                            target_size = (32, 32),\n",
    "                                            batch_size = 1,\n",
    "                                            shuffle = True,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            color_mode = 'rgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Very Simple Test Model\n",
    "\n",
    "To Test CROSS_VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_test_model():\n",
    "    '''creates an image classification model that uses 2 Convolutional CNNs layers (with maxpooling) and feeds the data through\n",
    "    a dense connected layer. This is a simple model to compute fast to test cross_validation'''\n",
    "    \n",
    "    cnn_model = keras.Sequential([\n",
    "\n",
    "        #convolutional layer with 32 3x3 filters - again, arbitrary,\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding = 'same', activation='relu'), \n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #2nd convolution with 128 filters\n",
    "        keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding = 'same', activation= 'relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "       \n",
    "        #MaxPooling - takes the max value of each 2x2 pool in the feature map\n",
    "        keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #the result of kthe CNN is then flattened and placed into the \n",
    "        keras.layers.Flatten(),\n",
    "        \n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #final layer, is output, 1 out of 4 possible results\n",
    "        #0 Normal, 1 Tapered, 2 Pyriform, 3 Amorphous\n",
    "        keras.layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define call-back early stopping criteria\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=300, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model: First try\n",
    "test_model = cnn_test_model()\n",
    "\n",
    "test_model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.7070 - accuracy: 0.7131 - val_loss: 20.8013 - val_accuracy: 0.5156\n",
      "Epoch 2/4000\n",
      "128/128 [==============================] - 4s 32ms/step - loss: 0.6860 - accuracy: 0.7302 - val_loss: 1.0778 - val_accuracy: 0.5469\n",
      "Epoch 3/4000\n",
      "128/128 [==============================] - 5s 37ms/step - loss: 0.6210 - accuracy: 0.7549 - val_loss: 1.0433 - val_accuracy: 0.6016\n",
      "Epoch 4/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.6113 - accuracy: 0.7549 - val_loss: 0.9794 - val_accuracy: 0.6094\n",
      "Epoch 5/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.5998 - accuracy: 0.7690 - val_loss: 0.8928 - val_accuracy: 0.6367\n",
      "Epoch 6/4000\n",
      "128/128 [==============================] - 4s 32ms/step - loss: 0.5729 - accuracy: 0.7686 - val_loss: 1.0363 - val_accuracy: 0.6328\n",
      "Epoch 7/4000\n",
      "128/128 [==============================] - 4s 32ms/step - loss: 0.5518 - accuracy: 0.7820 - val_loss: 0.9534 - val_accuracy: 0.6289\n",
      "Epoch 8/4000\n",
      "128/128 [==============================] - 4s 32ms/step - loss: 0.5116 - accuracy: 0.7993 - val_loss: 1.0104 - val_accuracy: 0.6016\n",
      "Epoch 9/4000\n",
      "128/128 [==============================] - 4s 32ms/step - loss: 0.5337 - accuracy: 0.7925 - val_loss: 0.9936 - val_accuracy: 0.6094\n",
      "Epoch 10/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.5239 - accuracy: 0.7979 - val_loss: 0.9882 - val_accuracy: 0.6172\n",
      "Epoch 11/4000\n",
      "128/128 [==============================] - 4s 32ms/step - loss: 0.4973 - accuracy: 0.8052 - val_loss: 0.9899 - val_accuracy: 0.6094\n",
      "Epoch 12/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.5340 - accuracy: 0.7898 - val_loss: 0.9432 - val_accuracy: 0.6055\n",
      "Epoch 13/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.5082 - accuracy: 0.8030 - val_loss: 0.9382 - val_accuracy: 0.6289\n",
      "Epoch 14/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.5054 - accuracy: 0.8035 - val_loss: 1.1870 - val_accuracy: 0.6094\n",
      "Epoch 15/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.4815 - accuracy: 0.8105 - val_loss: 1.0709 - val_accuracy: 0.6133\n",
      "Epoch 16/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.5511 - accuracy: 0.7917 - val_loss: 1.1089 - val_accuracy: 0.5898\n",
      "Epoch 17/4000\n",
      "128/128 [==============================] - 4s 35ms/step - loss: 0.5248 - accuracy: 0.7964 - val_loss: 1.1038 - val_accuracy: 0.6289\n",
      "Epoch 18/4000\n",
      "128/128 [==============================] - 5s 36ms/step - loss: 0.5015 - accuracy: 0.8074 - val_loss: 1.2091 - val_accuracy: 0.5859\n",
      "Epoch 19/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.5407 - accuracy: 0.7830 - val_loss: 1.2281 - val_accuracy: 0.6094\n",
      "Epoch 20/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.5348 - accuracy: 0.7935 - val_loss: 1.1525 - val_accuracy: 0.64840.5190 - accuracy:  - ETA: 3s - l - ETA: 0s - l\n",
      "Epoch 21/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.5362 - accuracy: 0.7925 - val_loss: 1.3823 - val_accuracy: 0.5000\n",
      "Epoch 22/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.5556 - accuracy: 0.7856 - val_loss: 1.1900 - val_accuracy: 0.6250\n",
      "Epoch 23/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.5014 - accuracy: 0.8140 - val_loss: 0.9848 - val_accuracy: 0.6250\n",
      "Epoch 24/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.5793 - accuracy: 0.7676 - val_loss: 1.4714 - val_accuracy: 0.4688\n",
      "Epoch 25/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.5767 - accuracy: 0.7668 - val_loss: 1.3311 - val_accuracy: 0.6133\n",
      "Epoch 26/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.5273 - accuracy: 0.7930 - val_loss: 1.5356 - val_accuracy: 0.6445\n",
      "Epoch 27/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.5062 - accuracy: 0.8049 - val_loss: 1.3610 - val_accuracy: 0.5117\n",
      "Epoch 28/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.5065 - accuracy: 0.8079 - val_loss: 1.7933 - val_accuracy: 0.6094\n",
      "Epoch 29/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.4508 - accuracy: 0.8284 - val_loss: 1.5090 - val_accuracy: 0.6016\n",
      "Epoch 30/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.4466 - accuracy: 0.8306 - val_loss: 2.5344 - val_accuracy: 0.5469\n",
      "Epoch 31/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.4568 - accuracy: 0.8201 - val_loss: 3.6434 - val_accuracy: 0.5781\n",
      "Epoch 32/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.4432 - accuracy: 0.8296 - val_loss: 1.2943 - val_accuracy: 0.6367\n",
      "Epoch 33/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.4390 - accuracy: 0.8320 - val_loss: 1.3586 - val_accuracy: 0.6289 - accu\n",
      "Epoch 34/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4393 - accuracy: 0.8315 - val_loss: 1.6797 - val_accuracy: 0.6680\n",
      "Epoch 35/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.4349 - accuracy: 0.8279 - val_loss: 1.6036 - val_accuracy: 0.6289\n",
      "Epoch 36/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.4318 - accuracy: 0.8337 - val_loss: 1.4976 - val_accuracy: 0.6133\n",
      "Epoch 37/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.4799 - accuracy: 0.8142 - val_loss: 1.8627 - val_accuracy: 0.6016\n",
      "Epoch 38/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4330 - accuracy: 0.8284 - val_loss: 1.1132 - val_accuracy: 0.6133\n",
      "Epoch 39/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4689 - accuracy: 0.8127 - val_loss: 1.2171 - val_accuracy: 0.6367\n",
      "Epoch 40/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.4470 - accuracy: 0.8254 - val_loss: 1.1438 - val_accuracy: 0.6523\n",
      "Epoch 41/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4262 - accuracy: 0.8354 - val_loss: 1.3568 - val_accuracy: 0.6016\n",
      "Epoch 42/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4185 - accuracy: 0.8391 - val_loss: 1.1798 - val_accuracy: 0.6016 - ac\n",
      "Epoch 43/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.5028 - accuracy: 0.8013 - val_loss: 1.3262 - val_accuracy: 0.6133\n",
      "Epoch 44/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.5509 - accuracy: 0.7815 - val_loss: 0.9379 - val_accuracy: 0.5703\n",
      "Epoch 45/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4757 - accuracy: 0.8201 - val_loss: 1.0314 - val_accuracy: 0.5547\n",
      "Epoch 46/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4595 - accuracy: 0.8235 - val_loss: 1.1236 - val_accuracy: 0.5898\n",
      "Epoch 47/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4475 - accuracy: 0.8313 - val_loss: 1.3726 - val_accuracy: 0.5117\n",
      "Epoch 48/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4847 - accuracy: 0.8113 - val_loss: 1.1735 - val_accuracy: 0.6016\n",
      "Epoch 49/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4491 - accuracy: 0.8259 - val_loss: 1.0367 - val_accuracy: 0.5664\n",
      "Epoch 50/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4284 - accuracy: 0.8362 - val_loss: 1.0531 - val_accuracy: 0.6250\n",
      "Epoch 51/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4213 - accuracy: 0.8455 - val_loss: 1.0399 - val_accuracy: 0.6289\n",
      "Epoch 52/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4210 - accuracy: 0.8447 - val_loss: 1.0251 - val_accuracy: 0.6289\n",
      "Epoch 53/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4455 - accuracy: 0.8284 - val_loss: 1.0170 - val_accuracy: 0.6172\n",
      "Epoch 54/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4087 - accuracy: 0.8489 - val_loss: 0.9912 - val_accuracy: 0.6250\n",
      "Epoch 55/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.3859 - accuracy: 0.8557 - val_loss: 1.1556 - val_accuracy: 0.5859\n",
      "Epoch 56/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4238 - accuracy: 0.8369 - val_loss: 1.5092 - val_accuracy: 0.5312\n",
      "Epoch 57/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.5344 - accuracy: 0.7983 - val_loss: 1.1769 - val_accuracy: 0.6094\n",
      "Epoch 58/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.4145 - accuracy: 0.8447 - val_loss: 1.0186 - val_accuracy: 0.6602\n",
      "Epoch 59/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.4102 - accuracy: 0.8423 - val_loss: 0.9967 - val_accuracy: 0.6602\n",
      "Epoch 60/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4107 - accuracy: 0.8428 - val_loss: 1.0165 - val_accuracy: 0.6289\n",
      "Epoch 61/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.3856 - accuracy: 0.8491 - val_loss: 1.1403 - val_accuracy: 0.5859\n",
      "Epoch 62/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.3966 - accuracy: 0.8459 - val_loss: 1.1481 - val_accuracy: 0.6211\n",
      "Epoch 63/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4184 - accuracy: 0.8440 - val_loss: 1.1612 - val_accuracy: 0.5781\n",
      "Epoch 64/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4032 - accuracy: 0.8459 - val_loss: 1.1847 - val_accuracy: 0.6172\n",
      "Epoch 65/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.3884 - accuracy: 0.8530 - val_loss: 1.1672 - val_accuracy: 0.5859\n",
      "Epoch 66/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.3865 - accuracy: 0.8530 - val_loss: 1.0345 - val_accuracy: 0.6562\n",
      "Epoch 67/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.3636 - accuracy: 0.8679 - val_loss: 1.2311 - val_accuracy: 0.6250\n",
      "Epoch 68/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.3679 - accuracy: 0.8633 - val_loss: 1.1207 - val_accuracy: 0.6406\n",
      "Epoch 69/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.3699 - accuracy: 0.8562 - val_loss: 1.0784 - val_accuracy: 0.6484\n",
      "Epoch 70/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4181 - accuracy: 0.8345 - val_loss: 1.2071 - val_accuracy: 0.6094\n",
      "Epoch 71/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.3610 - accuracy: 0.8674 - val_loss: 0.9914 - val_accuracy: 0.6680\n",
      "Epoch 72/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.3522 - accuracy: 0.8691 - val_loss: 1.1286 - val_accuracy: 0.6250\n",
      "Epoch 73/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.3623 - accuracy: 0.8650 - val_loss: 1.0977 - val_accuracy: 0.6367\n",
      "Epoch 74/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.3448 - accuracy: 0.8738 - val_loss: 1.0208 - val_accuracy: 0.6289\n",
      "Epoch 75/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.3741 - accuracy: 0.8572 - val_loss: 1.0866 - val_accuracy: 0.6289\n",
      "Epoch 76/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.4145 - accuracy: 0.8423 - val_loss: 1.1841 - val_accuracy: 0.6250\n",
      "Epoch 77/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.3749 - accuracy: 0.8569 - val_loss: 1.0691 - val_accuracy: 0.6289\n",
      "Epoch 78/4000\n",
      "128/128 [==============================] - 5s 36ms/step - loss: 0.3723 - accuracy: 0.8591 - val_loss: 1.0680 - val_accuracy: 0.6289\n",
      "Epoch 79/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.3517 - accuracy: 0.8674 - val_loss: 1.1119 - val_accuracy: 0.6289\n",
      "Epoch 80/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.3570 - accuracy: 0.8633 - val_loss: 1.2244 - val_accuracy: 0.6445\n",
      "Epoch 81/4000\n",
      "128/128 [==============================] - 4s 34ms/step - loss: 0.3517 - accuracy: 0.8638 - val_loss: 1.2418 - val_accuracy: 0.6523\n",
      "Epoch 82/4000\n",
      "128/128 [==============================] - 4s 33ms/step - loss: 0.3563 - accuracy: 0.8655 - val_loss: 1.1328 - val_accuracy: 0.6797\n",
      "Epoch 83/4000\n",
      "119/128 [==========================>...] - ETA: 0s - loss: 0.3392 - accuracy: 0.8726"
     ]
    }
   ],
   "source": [
    "#fit model\n",
    "history = test_model.fit(training_set,\n",
    "        epochs=4000,\n",
    "        validation_data=val_set,\n",
    "        verbose = 1, \n",
    "        validation_steps = len(val_set),\n",
    "        callbacks = [es])\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = test_model.evaluate(training_set, verbose=1)\n",
    "_, val_acc = test_model.evaluate(val_set, verbose=1)\n",
    "print('Train: %.3f, Val: %.3f' % (train_acc, val_acc))\n",
    "\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#saves model\n",
    "test_model.save(r\"C:\\Users\\Ricardo Santos\\Desktop\\Mestrado Ricardo\\Ano 1\\Spring Semester\\Deep\\Deep_Learning_Project\\Models\\test_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance do primeiro teste\n",
    "\n",
    "test_loss, test_acc = test_model.evaluate(test_set)\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Summary\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted probality\n",
    "prediction = test_model.predict(test_set,verbose=1)\n",
    "\n",
    "#Get class of prediction\n",
    "predicted_class = np.argmax(prediction,axis=1)\n",
    "\n",
    "# get names of pictures\n",
    "filenames = test_set.filenames\n",
    "\n",
    "#get trueclass\n",
    "true_classes = test_set.classes\n",
    "\n",
    "#store info in dataframe\n",
    "df_predictions = pd.DataFrame({'Filename': filenames, 'Label': true_classes, 'Test': predicted_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and classification report\n",
    "class_labels = list(test_set.class_indices.keys())   \n",
    "print(confusion_matrix(true_classes, predicted_class))\n",
    "\n",
    "report = classification_report(true_classes, predicted_class, target_names=class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Design a More Complex and Robust Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_cnn_standard_model():\n",
    "    '''creates an image classification model that uses 2 Convolutional CNNs layers (with maxpooling) and feeds the data through\n",
    "    a dense connected layer. This is trial and error there is no specific reason for 2 layers'''\n",
    "    cnn_model = keras.Sequential([\n",
    "\n",
    "        #convolutional layer with 32 3x3 filters - again, arbitrary,\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding = 'same', activation='relu'), \n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #MaxPooling - takes the max value of each 2x2 pool in the feature map\n",
    "        keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #3rd convolution with 64 filters\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3,3),  padding = 'same', activation= 'relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #4rd convolution with 128 filters\n",
    "        keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding = 'same', activation= 'relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #Second MaxPool2D - to check with other options\n",
    "        keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #5th convolutional layer with 24 3x3 filters - again, arbitrary,\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=(4,4), padding = 'same', activation='relu'), \n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #MaxPooling - takes the max value of each 2x2 pool in the feature map\n",
    "        #keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "\n",
    "        #second convolution with 36 filters\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding = 'same', activation= 'relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding = 'same', activation= 'relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #Second MaxPool2D - to check with other options\n",
    "        keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #convolutional layer with 24 3x3 filters - again, arbitrary,\n",
    "        keras.layers.Conv2D(filters=128, kernel_size=(4,4), padding = 'same', activation='relu'), \n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "\n",
    "        #second convolution with 36 filters\n",
    "        keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding = 'same', activation= 'relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding = 'same', activation= 'relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #Second MaxPool2D - to check with other options\n",
    "        keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        #the result of kthe CNN is then flattened and placed into the \n",
    "        keras.layers.Flatten(),\n",
    "        \n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        #Add Dropout\n",
    "        keras.layers.Dropout(0.4),\n",
    "        \n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        #final layer, is output, 1 out of 5 possible results\n",
    "        #0 Normal, 1 Tapered, 2 Pyriform, 3 Amorphous\n",
    "        keras.layers.Dense(4, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model: First try\n",
    "cnn_model = image_cnn_standard_model()\n",
    "\n",
    "cnn_model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fit model\n",
    "history = cnn_model.fit(training_set,\n",
    "        epochs=4000,\n",
    "        validation_data=val_set,\n",
    "        verbose = 1, \n",
    "        validation_steps = len(val_set),\n",
    "        callbacks = [es])\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = cnn_model.evaluate(training_set, verbose=1)\n",
    "_, val_acc = cnn_model.evaluate(val_set, verbose=1)\n",
    "print('Train: %.3f, Val: %.3f' % (train_acc, val_acc))\n",
    "\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "cnn_model.save(r\"C:\\Users\\Ricardo Santos\\Desktop\\Mestrado Ricardo\\Ano 1\\Spring Semester\\Deep\\Deep_Learning_Project\\Models\\model_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Summary\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test performance\n",
    "\n",
    "test_loss, test_acc = cnn_model.evaluate(test_set)\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions\n",
    "\n",
    "Code still needs improvement\n",
    "\n",
    "These predictions are gated by batch size\n",
    "\n",
    "Let's try and improve it with bruteforce Hyperparam optimization whiole being on the lookout for other ways to improve performance. Model is predicting all as Amorphous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted probality\n",
    "prediction = cnn_model.predict(test_set,verbose=1)\n",
    "\n",
    "#Get class of prediction\n",
    "predicted_class = np.argmax(prediction,axis=1)\n",
    "\n",
    "#get trueclass\n",
    "true_classes = test_set.classes\n",
    "\n",
    "# get names of pictures\n",
    "filenames = test_set.filenames\n",
    "\n",
    "#store info in dataframe\n",
    "robust_predictions = pd.DataFrame({'Filename': filenames,'Model_1': predicted_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join prediction to dataframe\n",
    "df_predictions = df_predictions.merge(robust_predictions, on = 'Filename', how = 'inner')\n",
    "\n",
    "del robust_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and classification report\n",
    "class_labels = list(test_set.class_indices.keys())   \n",
    "print(confusion_matrix(true_classes, predicted_class))\n",
    "\n",
    "report = classification_report(true_classes, predicted_class, target_names=class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 - Benchmarks and other tests\n",
    "\n",
    "Try and compare the models performance with other standardly used models:\n",
    "\n",
    "AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALEX NET\n",
    "\n",
    "#Importing library \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "#Instantiation\n",
    "AlexNet = Sequential()\n",
    "\n",
    "#1st Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=96, input_shape=(32,32,3), kernel_size=(11,11), strides=(4,4), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#2nd Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#3rd Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "#4th Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "#5th Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "#Passing it to a Fully Connected layer\n",
    "AlexNet.add(Flatten())\n",
    "# 1st Fully Connected Layer\n",
    "AlexNet.add(Dense(4096, input_shape=(32,32,3,)))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "#2nd Fully Connected Layer\n",
    "AlexNet.add(Dense(4096))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "#Add Dropout\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "#3rd Fully Connected Layer\n",
    "AlexNet.add(Dense(1000))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "#Add Dropout\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "#Output Layer\n",
    "AlexNet.add(Dense(4))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('softmax'))\n",
    "\n",
    "#Model Summary\n",
    "AlexNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "AlexNet.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "\n",
    "history = AlexNet.fit(training_set,\n",
    "        epochs=4000,\n",
    "        validation_data=val_set,\n",
    "        verbose = 1, \n",
    "        validation_steps = len(val_set),\n",
    "        callbacks = [es])\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = AlexNet.evaluate(training_set, verbose=1)\n",
    "_, val_acc = AlexNet.evaluate(val_set, verbose=1)\n",
    "print('Train: %.3f, Val: %.3f' % (train_acc, val_acc))\n",
    "\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "AlexNet.save(r\"C:\\Users\\Ricardo Santos\\Desktop\\Mestrado Ricardo\\Ano 1\\Spring Semester\\Deep\\Deep_Learning_Project\\Models\\Alex.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = AlexNet.evaluate(test_set)\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted probality\n",
    "prediction = AlexNet.predict(test_set,verbose=1)\n",
    "\n",
    "#Get class of prediction\n",
    "predicted_class = np.argmax(prediction,axis=1)\n",
    "\n",
    "#get trueclass\n",
    "true_classes = test_set.classes\n",
    "\n",
    "# get names of pictures\n",
    "filenames = test_set.filenames\n",
    "\n",
    "#store info in dataframe\n",
    "alex_predictions = pd.DataFrame({'Filename': filenames,'AlexNet': predicted_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join prediction to dataframe\n",
    "df_predictions = df_predictions.merge(alex_predictions, on = 'Filename', how = 'inner')\n",
    "\n",
    "del alex_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and classification report\n",
    "class_labels = list(test_set.class_indices.keys())   \n",
    "print(confusion_matrix(true_classes, predicted_class))\n",
    "\n",
    "report = classification_report(true_classes, predicted_class, target_names=class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning with RESNET50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet50 requires its own preprocessing \n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "#to play around with these\n",
    "res50train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   preprocessing_function=preprocess_input,\n",
    "                                   rotation_range = 5,\n",
    "                                   width_shift_range = 0.06, \n",
    "                                   height_shift_range = 0.06, \n",
    "                                   vertical_flip = True,\n",
    "                                   horizontal_flip = True,\n",
    "                                   brightness_range=[0.2,1.2], \n",
    "                                   fill_mode='nearest',\n",
    "                                   zoom_range = 0.2,\n",
    "                                   ) \n",
    "\n",
    "res50val_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function=preprocess_input)\n",
    "res50test_datagen = ImageDataGenerator(rescale = 1./255, preprocessing_function=preprocess_input)\n",
    "\n",
    "#test different color maps -  class modes and cross validation types\n",
    "res_train = res50train_datagen.flow_from_directory(path+'/train',\n",
    "                                                 target_size = (32, 32),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode=\"categorical\",\n",
    "                                                 color_mode = 'rgb',\n",
    "                                                 shuffle = True)\n",
    "\n",
    "res_val = res50val_datagen.flow_from_directory(path+'/val',\n",
    "                                            target_size = (32, 32),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            color_mode = 'rgb',\n",
    "                                            shuffle = True)\n",
    "\n",
    "res_test = res50test_datagen.flow_from_directory(path+'/test',\n",
    "                                            target_size = (32, 32),\n",
    "                                            batch_size = 1,\n",
    "                                            class_mode=\"categorical\",\n",
    "                                            color_mode = 'rgb',\n",
    "                                            shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "\n",
    "res50model = Sequential()\n",
    "\n",
    "#Layer 1: RES50 without top layer\n",
    "res50model.add(ResNet50(weights='imagenet', input_shape= (32,32,3),\n",
    "                 include_top = False, classes=5,))\n",
    "\n",
    "#Layer 2: RES50 without top layer\n",
    "res50model.add(Flatten())\n",
    "res50model.add(Dense(4, activation = 'softmax'))\n",
    "\n",
    "#freeze layers in resnet - weights obtained with IMAGENET challenge, we only train final layer\n",
    "res50model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile\n",
    "\n",
    "res50model.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "\n",
    "res_history = res50model.fit(res_train,\n",
    "        epochs=4000,\n",
    "        validation_data=res_val,\n",
    "        verbose = 1, \n",
    "        validation_steps = len(val_set),\n",
    "        callbacks = [es])\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = res50model.evaluate(training_set, verbose=1)\n",
    "_, val_acc = res50model.evaluate(val_set, verbose=1)\n",
    "print('Train: %.3f, Val: %.3f' % (train_acc, val_acc))\n",
    "\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calling `save('res50_net.h5')`.\n",
    "res50model.save(r\"C:\\Users\\Ricardo Santos\\Desktop\\Mestrado Ricardo\\Ano 1\\Spring Semester\\Deep\\Deep_Learning_Project\\Models\\Res50_net.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = res50model.evaluate(res_test)\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted probality\n",
    "prediction = res50model.predict(res_test,verbose=1)\n",
    "\n",
    "#Get class of prediction\n",
    "predicted_class = np.argmax(prediction,axis=1)\n",
    "\n",
    "#get trueclass\n",
    "true_classes = res_test.classes\n",
    "\n",
    "# get names of pictures\n",
    "filenames = res_test.filenames\n",
    "\n",
    "#store info in dataframe\n",
    "res50_prebuilt_predictions = pd.DataFrame({'Filename': filenames,'ResNet50': predicted_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join prediction to dataframe\n",
    "df_predictions = df_predictions.merge(res50_prebuilt_predictions, on = 'Filename', how = 'inner')\n",
    "\n",
    "del res50_prebuilt_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and classification report\n",
    "class_labels = list(test_set.class_indices.keys())   \n",
    "print(confusion_matrix(true_classes, predicted_class))\n",
    "\n",
    "report = classification_report(true_classes, predicted_class, target_names=class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESNET50 Trained from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50model_2 = Sequential()\n",
    "\n",
    "#Layer 1: RES50 without top layer\n",
    "res50model_2.add(ResNet50(weights=None, input_shape= (32,32,3),\n",
    "                 include_top = False, classes=5,))\n",
    "\n",
    "#Layer 2: RES50 without top layer\n",
    "res50model_2.add(Flatten())\n",
    "res50model_2.add(Dense(4, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile\n",
    "\n",
    "res50model_2.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "\n",
    "res_history_2 = res50model_2.fit(res_train,\n",
    "        epochs=4000,\n",
    "        validation_data=res_val,\n",
    "        verbose = 1, \n",
    "        validation_steps = len(res_val),\n",
    "        callbacks = [es])\n",
    "\n",
    "# evaluate the model\n",
    "_, train_acc = res50model_2.evaluate(res_train, verbose=1)\n",
    "_, val_acc = res50model_2.evaluate(res_val, verbose=1)\n",
    "print('Train: %.3f, Val: %.3f' % (train_acc, val_acc))\n",
    "\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calling `save('my_model.h5')` creates a h5 file `my_model.h5`.\n",
    "res50model.save(r\"C:\\Users\\Ricardo Santos\\Desktop\\Mestrado Ricardo\\Ano 1\\Spring Semester\\Deep\\Deep_Learning_Project\\Models\\Res50_from_scratch.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performance do primeiro teste\n",
    "\n",
    "test_loss, test_acc = res50model_2.evaluate(res_test)\n",
    "\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predicted probality\n",
    "prediction = res50model_2.predict(res_test,verbose=1)\n",
    "\n",
    "#Get class of prediction\n",
    "predicted_class = np.argmax(prediction,axis=1)\n",
    "\n",
    "#get trueclass\n",
    "true_classes = res_test.classes\n",
    "\n",
    "# get names of pictures\n",
    "filenames = res_test.filenames\n",
    "\n",
    "#store info in dataframe\n",
    "res50_predictions = pd.DataFrame({'Filename': filenames,'ResNet50 From Scratch': predicted_class})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join prediction to dataframe\n",
    "df_predictions = df_predictions.merge(res50_predictions, on = 'Filename', how = 'inner')\n",
    "\n",
    "del res50_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and classification report\n",
    "class_labels = list(test_set.class_indices.keys())   \n",
    "print(confusion_matrix(true_classes, predicted_class))\n",
    "\n",
    "report = classification_report(true_classes, predicted_class, target_names=class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FOR TOMORROW\n",
    "\n",
    "Check different loss functions\n",
    "CROSS VALIDATION\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
